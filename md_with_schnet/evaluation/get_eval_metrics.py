import os
import argparse
import torch
import pytorch_lightning as pl
import numpy as np
import pandas as pd
import torch.multiprocessing as mp
mp.set_sharing_strategy('file_system') # to avoid problems with num_workers > 0

from ase import Atoms
from xtb_ase.calculator import XTB
from omegaconf import OmegaConf, DictConfig
from schnetpack.utils.compatibility import load_model
from tqdm import tqdm

from md_with_schnet.units import convert_energies, convert_forces, get_ase_units_from_str
from md_with_schnet.utils import set_data_prefix, get_split_path, load_config, setup_datamodule, get_num_workers, set_data_units_in_config
from md_with_schnet.setup_logger import setup_logger

BATCH_SIZE = 1

# Example command to run the script from within code directory:
"""
python -m md_with_schnet.evaluation.get_eval_metrics -mdir epochs_1000_bs_100_lr_0.0001_flw_389830.46_elw_3937.68_seed_42 --units angstrom_hartree_fs --evaluation_data test
"""


logger = setup_logger("debug")

def parse_args() -> dict:
    """ 
    Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for predicting with trained model on XTB test data.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="MOTOR_MD_XTB/T300_1", help="Directory containing the trajectory data generated by Turbomole (default: MOTOR_MD_XTB/T300_1)")
    parser.add_argument("--units", type=str, default="angstrom_kcal_per_mol_fs", choices=["angstrom_kcal_per_mol_fs", "angstrom_ev_fs", "angstrom_hartree_fs", "bohr_hartree_aut"], help="Units for the input data (default: angstrom_kcal_per_mol_fs).")
    parser.add_argument("-mdir", "--model_dir", type=str, default="epochs_1000_bs_100_lr_0.0001_seed_42", help="Directory of the trained model (default: epochs_1000_bs_100_lr_0.0001_seed_42)")
    parser.add_argument("--evaluation_data", type=str, default="test", choices=["train", "val", "test"], help="Data to evaluate the model on (default: test)")
    parser.add_argument("-f", "--fold", type=int, default=0, help="Fold number for cross-validation (default: 0)")
    parser.add_argument("-s", "--seed", type=int, default=42, help="Random seed for reproducibility (default: 42)")
    parser.add_argument("-nw", "--num_workers", type=int, default=-1, help="Number of workers for data loading (default: -1, which sets it to 0 on macOS and 8 on Linux)")
    parser.add_argument("--compute_xtb", action="store_true", help="Whether to compute XTB energies and forces")
    return vars(parser.parse_args())



def update_config_with_train_config(cfg: DictConfig, cfg_train: DictConfig) -> DictConfig:
    """
    Update the configuration with the training configuration to ensure consistency.
    Args:
        cfg (DictConfig): The original configuration.
        cfg_train (DictConfig): The training configuration.
    Returns:
        DictConfig: Updated configuration.
    """
    # Disable struct mode to allow adding new fields
    OmegaConf.set_struct(cfg, False)

    # Copy over specific globals and of run
    cfg.globals.cutoff = cfg_train.globals.cutoff
    cfg.globals.energy_key = cfg_train.globals.energy_key
    cfg.globals.forces_key = cfg_train.globals.forces_key
    cfg.run.mean_std_path = cfg_train.run.mean_std_path

    # Copy over data fields
    cfg.data = cfg_train.data
    # Create model field if it doesn't exist
    if 'model' not in cfg:
        cfg.model = DictConfig({})
    # Copy over model fields
    cfg.model.postprocessors = cfg_train.model.postprocessors

    # Re-enable struct mode to enforce the structure
    OmegaConf.set_struct(cfg, True)
    return cfg

def update_config(cfg: DictConfig, ase_unit_names: dict, run_path: str, num_workers: int) -> DictConfig:
    """ 
    Update the configuration with command-line arguments.
    Args:
        cfg (DictConfig): The original configuration.
        ase_unit_names (dict): A dictionary containing the ASE unit names for distance, energy, and forces.
        run_path (str): Path to save the run.
        num_workers (int): Number of workers for data loading.
    Returns:
        DictConfig: Updated configuration.
    """
    cfg.run.path = run_path
    cfg.data.num_workers = get_num_workers(num_workers)
    cfg.org_data.batch_size = BATCH_SIZE  # Set batch size for inference

    # Set ASE units in the configuration
    cfg.org_data = set_data_units_in_config(cfg.org_data, ase_unit_names)

    return cfg

def collect_predictions_and_ground_truths(cfg: DictConfig, datamodule: pl.LightningDataModule, model: torch.nn.Module):
    """ Collect predictions and ground truths for energy and forces from the neural network model on the test data.
    Args:
        cfg (DictConfig): Configuration containing model and data parameters.
        datamodule: Data module containing the test data.
        model: The trained neural network model.
    Returns:
        dict: A dictionary containing predictions and ground truths for energy and forces.
    """
    results = {
        "predictions": {"energy": [], "forces": []}, 
        "ground_truths": {"energy": [], "forces": []}
        }
    for batch in tqdm(datamodule, desc="Evaluating model on test data"): 
        e_gt = batch[cfg.globals.energy_key].cpu().numpy()
        f_gt = batch[cfg.globals.forces_key].cpu().numpy()
        # forward pass (gradients needed for forces)
        output = model(batch)
        # energy and forces required grads -> detach them
        e_pred = output[cfg.globals.energy_key].detach().cpu().numpy()
        f_pred = output[cfg.globals.forces_key].detach().cpu().numpy()
        # store predictions and ground truths
        results["predictions"]["energy"].append(e_pred)
        results["predictions"]["forces"].append(f_pred)
        results["ground_truths"]["energy"].append(e_gt)
        results["ground_truths"]["forces"].append(f_gt)

    return results

def transform_predictions_and_ground_truths_to_numpy(preds_and_gts: dict, nr_atoms: int) -> dict:
    """ Transform predictions and ground truths from lists to numpy arrays.
    Args:
        preds_and_gts (dict): A dictionary containing predictions and ground truths for energy and forces.
        nr_atoms (int): Number of atoms in each sample.
    Returns:
        dict: A dictionary containing predictions and ground truths as numpy arrays.
    """
    transformed = {
        "predictions": {
            "energy": np.concatenate(preds_and_gts["predictions"]["energy"]),
            "forces": np.concatenate(preds_and_gts["predictions"]["forces"]).reshape(-1, nr_atoms, 3)  
        },
        "ground_truths": {
            "energy": np.concatenate(preds_and_gts["ground_truths"]["energy"]),
            "forces": np.concatenate(preds_and_gts["ground_truths"]["forces"]).reshape(-1, nr_atoms, 3)
        }
    }
    logger.debug(f"Transformed predictions shape: {transformed['predictions']['energy'].shape}, {transformed['predictions']['forces'].shape}")
    logger.debug(f"Transformed ground truths shape: {transformed['ground_truths']['energy'].shape}, {transformed['ground_truths']['forces'].shape}")
    return transformed

def check_shapes(preds_and_gts: dict, nr_samples: int):
    """ Check the shapes of predictions and ground truths.
    Args:
        preds_and_gts (dict): A dictionary containing predictions and ground truths for energy and forces.
        num_samples (int): Number of samples to check.
    """
    assert preds_and_gts["predictions"]["energy"].shape[0] == nr_samples, \
        f"Predicted energies shape mismatch: {preds_and_gts['predictions']['energy'].shape[0]} != {nr_samples}"
    assert preds_and_gts["predictions"]["forces"].shape[0] == nr_samples, \
        f"Predicted forces shape mismatch: {preds_and_gts['predictions']['forces'].shape[0]} != {nr_samples}"
    assert preds_and_gts["ground_truths"]["energy"].shape[0] == nr_samples, \
        f"Ground truth energies shape mismatch: {preds_and_gts['ground_truths']['energy'].shape[0]} != {nr_samples}"
    assert preds_and_gts["ground_truths"]["forces"].shape[0] == nr_samples, \
        f"Ground truth forces shape mismatch: {preds_and_gts['ground_truths']['forces'].shape[0]} != {nr_samples}"

def compute_mae_with_std_error(preds_and_gts: dict, property: str) -> tuple:
    """ Compute Mean Absolute Error (MAE) and its standard error for a given property.
    Args:
        preds_and_gts (dict): A dictionary containing predictions and ground truths for energy and forces.
        property (str): The property to compute the metrics for ("energy" or "forces").
    Returns:
        tuple: Mean Absolute Error (MAE) and its standard error for the specified property.
    """
    abs_diff = np.abs(preds_and_gts["predictions"][property] - preds_and_gts["ground_truths"][property])
    mae = np.mean(abs_diff)
    mae_std_error = np.std(abs_diff) / np.sqrt(len(abs_diff))  # Standard error of the mean

    return mae, mae_std_error

def compute_metrics_for_xtb(cfg: DictConfig, datamodule: pl.LightningDataModule):
    """ Compute metrics for the XTB model on the test data.
    Args:
        cfg (DictConfig): Configuration containing model and data parameters.
        datamodule: Data module containing the test data.
    Returns:
        tuple: Mean Absolute Error (MAE) for forces and energy.
    """
    forces_mae = energy_mae = 0
    for batch in tqdm(datamodule.test_dataloader(), desc="Evaluating model on test data"):
        nums = batch["_atomic_numbers"].numpy()
        pos = batch["_positions"].numpy()   
        e_gt = batch[cfg.globals.energy_key].numpy() 
        f_gt = batch[cfg.globals.forces_key].numpy()

        atoms = Atoms(numbers=nums, positions=pos)
        atoms.calc = XTB(method=cfg.xtb.method)
        e_pred = atoms.get_potential_energy() # eV
        f_pred = atoms.get_forces() # eV/Angstrom 
            
        # compute metrics
        forces_mae += get_batch_mae(f_pred, f_gt)
        energy_mae += get_batch_mae(e_pred, e_gt)

    forces_mae, energy_mae = average_over_batches(datamodule, forces_mae, energy_mae)

    return forces_mae, energy_mae

def average_over_batches(datamodule: pl.LightningDataModule, forces_mae: float, energy_mae: float):
    """ 
    Average the computed metrics over all batches.
    Args:
        datamodule: Data module containing the test data.
        forces_mae (float): Mean Absolute Error for forces.
        energy_mae (float): Mean Absolute Error for energy.
    Returns:
        tuple: Averaged Mean Absolute Error (MAE) for forces and energy.
    """
    num_batches = len(datamodule.test_dataloader())
    return forces_mae / num_batches, energy_mae / num_batches

def get_batch_mae(pred: np.ndarray, gt: np.ndarray) -> float:
    """ Compute Mean Absolute Error (MAE) for a batch.
    Args:
        pred (np.ndarray): Predicted values.
        gt (np.ndarray): Ground truth values.
    Returns:
        float: Mean Absolute Error for the batch.
    """
    return np.mean(np.abs(pred - gt)).item()

    
def main(trajectory_dir: str, units: str, model_dir: str, evaluation_data: str, fold: int, seed: int, num_workers: int, compute_xtb: bool):
    pl.seed_everything(seed, workers=True)

    ####################### 1) Compose the config ###########################
    cfg = load_config("training_and_inference/conf", "inference_config", "inference")

    # set absolute and relative paths to the model directory
    home_dir = os.path.expanduser("~")
    runs_dir_path = os.path.join(home_dir, cfg.globals.runs_dir_subpath)
    model_dir_path = os.path.join(runs_dir_path, units, trajectory_dir, model_dir)
    model_dir_rel_path = "".join(model_dir_path.split("md_with_schnet/")[1:])
    logger.debug(f"Absolute path to the model directory: {model_dir_path}")

    # use training config to update the inference config
    train_cfg_path = os.path.join(model_dir_rel_path, cfg.globals.train_config_subpath)
    cfg_train = load_config(train_cfg_path, cfg.globals.hparams_file_name, "train")
    cfg = update_config_with_train_config(cfg, cfg_train)

    # update config with arguments from command line
    model_units = get_ase_units_from_str(units)
    cfg = update_config(cfg, model_units, model_dir_path, num_workers)
    logger.info(f"Loaded and updated config:\n{OmegaConf.to_yaml(cfg)}")

    ####################### 2) Prepare Data and Paths #########################
    data_prefix = set_data_prefix()
    split_file = get_split_path(data_prefix, trajectory_dir, fold)
    path_to_db = os.path.join(data_prefix, trajectory_dir, f"md_trajectory_{units}.db")
    
    datamodule = setup_datamodule(data_cfg=cfg.org_data, datapath=path_to_db, split_file=split_file)
    if evaluation_data == "train":
        datamodule = datamodule.train_dataloader()
    elif evaluation_data == "val":
        datamodule = datamodule.val_dataloader()
    elif evaluation_data == "test":
        datamodule = datamodule.test_dataloader()


    len_test = len(datamodule.dataset)
    nr_atoms = datamodule.dataset[0]["_positions"].shape[0]
    logger.info(f"Number of {evaluation_data} samples: {len_test}")
    logger.info(f"Number of atoms in each sample: {nr_atoms}")
    
    ####################### 3) Collect predictions and ground truths of the test dataset ##############################    
    model = load_model(cfg.globals.model_path, device="cpu")
    model.eval()

    preds_and_gts = collect_predictions_and_ground_truths(cfg, datamodule, model)
    preds_and_gts = transform_predictions_and_ground_truths_to_numpy(preds_and_gts, nr_atoms)
    check_shapes(preds_and_gts, len_test)

    ######################## 4) Compute metrics  ##############################
    logger.info("Computing metrics for the neural network model...")
    nn_energy_mae, nn_energy_mae_std_err = compute_mae_with_std_error(preds_and_gts, "energy")
    nn_forces_mae, nn_forces_mae_std_err = compute_mae_with_std_error(preds_and_gts, "forces")

    if compute_xtb:
        xtb_forces_mae, xtb_energy_mae = compute_metrics_for_xtb(cfg, datamodule)
        logger.info(f"Test metrics for XTB:")
        logger.info(f"Energy MAE: {xtb_energy_mae:.6f} eV")
        logger.info(f"Forces MAE: {xtb_forces_mae:.6f} eV/Angstrom")

    ######################### 5) Converts metrics to internal ASE units and kcal/mol ##############################
    ASE_ENERGY_UNIT = "ev"
    ASE_FORCE_UNIT = "ev/angstrom"

    model_energy_unit = model_units['energy'].lower()
    model_forces_unit = model_units['forces'].lower()
    logger.info(f"Model energy unit: {model_energy_unit}, Model forces unit: {model_forces_unit}")
    logger.info(f"ASE energy unit: {ASE_ENERGY_UNIT}, ASE forces unit: {ASE_FORCE_UNIT}")

    nn_energy_mae = convert_energies(nn_energy_mae, model_energy_unit, ASE_ENERGY_UNIT)
    nn_energy_mae_std_err = convert_energies(nn_energy_mae_std_err, model_energy_unit, ASE_ENERGY_UNIT)
    nn_forces_mae = convert_forces(nn_forces_mae, model_forces_unit, ASE_FORCE_UNIT)
    nn_forces_mae_std_err = convert_forces(nn_forces_mae_std_err, model_forces_unit, ASE_FORCE_UNIT)
    nn_energy_mae_kcal = convert_energies(nn_energy_mae, ASE_ENERGY_UNIT, 'kcal/mol')
    nn_energy_mae_std_err_kcal = convert_energies(nn_energy_mae_std_err, ASE_ENERGY_UNIT, 'kcal/mol')
    nn_forces_mae_kcal = convert_forces(nn_forces_mae, ASE_FORCE_UNIT, 'kcal/mol/angstrom')
    nn_forces_mae_std_err_kcal = convert_forces(nn_forces_mae_std_err, ASE_FORCE_UNIT, 'kcal/mol/angstrom')

    ######################### 6) Log metrics and save them to a csv file ##############################
    # log energies in ev/angstrom and kcal/mol
    logger.info(f"Metrics on {evaluation_data} for nn:")
    logger.info(f"Energy MAE: {nn_energy_mae:.6f} +/- {nn_energy_mae_std_err:.6f} eV")
    logger.info(f"Forces MAE: {nn_forces_mae:.6f} +/- {nn_forces_mae_std_err:.6f} eV/Angstrom")
    logger.info(f"Energy MAE: {nn_energy_mae_kcal:.6f} +/- {nn_energy_mae_std_err_kcal:.6f} kcal/mol")
    logger.info(f"Forces MAE: {nn_forces_mae_kcal:.6f} +/- {nn_forces_mae_std_err_kcal:.6f} kcal/mol/Angstrom")

    # Save the metrics to a csv file
    results = {
        f"energy_mae_{ASE_ENERGY_UNIT}": nn_energy_mae,
        f"energy_mae_std_err_{ASE_ENERGY_UNIT}": nn_energy_mae_std_err,
        f"forces_mae_{ASE_FORCE_UNIT.replace("/", "_per_")}": nn_forces_mae,
        f"forces_mae_std_err_{ASE_FORCE_UNIT.replace("/", "_per_")}": nn_forces_mae_std_err,
        f"energy_mae_kcal_per_mol": nn_energy_mae_kcal,
        f"energy_mae_std_err_kcal_per_mol": nn_energy_mae_std_err_kcal,
        f"forces_mae_kcal_per_mol_per_angstrom": nn_forces_mae_kcal,
        f"forces_mae_std_err_kcal_per_mol_per_angstrom": nn_forces_mae_std_err_kcal
    }
    metrics_file = os.path.join(model_dir_path, f"{evaluation_data}_metrics_fold_{fold}.csv")
    df = pd.DataFrame([results])
    df.to_csv(metrics_file, index=False)
    logger.info(f"Metrics on {evaluation_data} saved to {metrics_file}")


if __name__ == "__main__":
    args = parse_args()
    main(**args)

