import argparse
import os
import pytorch_lightning as pl
import schnetpack as spk
import schnetpack.transform as trn
import torch
import platform
from tqdm import tqdm

from md_with_schnet.setup_logger import setup_logger
from md_with_schnet.utils import set_data_prefix
from md_with_schnet.preprocessing.standarize_energy_forces import StandardizeEnergy, StandardizeForces

logger = setup_logger("debug")
# set batch size to 1, since batches are loaded weirdly (bs*num_atoms, 3) 
BATCH_SIZE = 1

# Example command to run the script from within code directory:
"""
python -m md_with_schnet.preprocessing.compute_means_and_stds --trajectory_dir MOTOR_MD_XTB/T300_1 --num_atoms=48
"""


def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for preparing and scaling data.")
    parser.add_argument("--trajectory_dir", type=str, default="MOTOR_MD_XTB/T300_1", help="Directory containing the trajectory data generated by Turbomole (default: MOTOR_MD_XTB/T300_1)")
    parser.add_argument("--fold", type=int, default=0, help="Fold number for cross-validation (default: 0)")
    parser.add_argument('--num_atoms', type=int, required=True, help='Number of atoms in the simulation. Can be found in the "control" file under "natoms".')
    return vars(parser.parse_args())


def get_split_path(data_prefix: str, trajectory_dir: str, fold: int = 0) -> str:
    """
    Get the path to the split file for the given trajectory directory and fold.
    Args:
        data_prefix (str): The prefix path to the data directory.
        trajectory_dir (str): The directory containing the trajectory data.
        fold (int): The fold number for cross-validation (default: 0).
    Returns:
        str: The path to the split file.
    """
    split_file = os.path.join(data_prefix, "splits", trajectory_dir, f"inner_splits_{fold}.npz")
    if not os.path.exists(split_file):
        raise FileNotFoundError(f"Missing split file: {split_file}")
    logger.debug(f"Split file: {split_file}")
    return split_file

def load_xtb_dataset(db_path: str, num_workers: int, batch_size: int, transforms: list, split_file: str | None = None, pin_memory: bool | None = None) -> spk.data.datamodule.AtomsDataModule:
    """
    Load an XTB dataset from the specified path. 
    Note: data.prepare_data() and data.setup() do not need to be called here, since they will be called by pl.trainer.fit().
    Args:
        db_path (str): Path to the dataset.
        num_workers (int): Number of workers for data loading.
        batch_size (int): Batch size for the dataset.
        split_file (str | None): Path to the split file. Default is None.
        pin_memory (bool | None): Whether to use pinned memory. Default is None.
    Returns:
        spk.data.datamodule.AtomsDataModule: The loaded XTB dataset.
    """
    if pin_memory is None:
        pin_memory = torch.cuda.is_available()

    if num_workers == -1:
        num_workers = 0 if platform.system() == "Darwin" else 31
    else:
        num_workers = num_workers
    logger.debug(f"pin_memory: {pin_memory}")
    logger.debug(f"num_workers: {num_workers}")

    # load xtb dataset with subclass of pl.LightningDataModule
    data = spk.data.AtomsDataModule(
        db_path,
        batch_size=batch_size,
        distance_unit='Ang',
        property_units={'energy':'Hartree', 'forces':'Hartree/Bohr'},
        split_file=split_file,
        transforms=transforms,
        num_workers=num_workers,
        pin_memory=pin_memory, # set to false, when not using a GPU
    )
    data.prepare_data()
    data.setup()
    logger.info(f"loaded xtb dataset: {data}")

    if not isinstance(data, pl.LightningDataModule):
        raise ValueError("The loaded dataset is not an instance of pl.LightningDataModule. Please check the dataset path and configuration.")

    return data

def get_splits_and_load_data(data_prefix: str, trajectory_dir: str, num_workers: int, batch_size: int, transforms: list, fold: int = 0) -> pl.LightningDataModule:
    """
    Prepare loading the dataset and then load it.
    Args:
        data_prefix (str): The prefix path to the data directory.
        trajectory_dir (str): The directory containing the trajectory data.
        num_workers (int): The number of workers for data loading.
        batch_size (int): The batch size for data loading.
        transforms (list): List of transforms to apply to the dataset.
        fold (int): The fold number for cross-validation (default: 0).
    Raises:
        FileNotFoundError: If the split file does not exist.
    Returns:
        pl.LightningDataModule: The data module containing the dataset.
    """
    split_file = get_split_path(data_prefix, trajectory_dir, fold)

    if not os.path.exists(split_file):
        raise FileNotFoundError(f"Split file does not exist: {split_file}, try running the create_splits.py script first.")

    path_to_db = os.path.join(data_prefix, trajectory_dir, "md_trajectory.db")
    logger.debug(f"Path to database: {path_to_db}")

    datamodule = load_xtb_dataset(
        db_path=path_to_db,
        num_workers=num_workers,
        batch_size=batch_size,
        transforms=transforms,
        split_file=split_file
    )
    return datamodule

def load_data_and_compute_stats(data_prefix: str, trajectory_dir: str, device: torch.device, fold: int, num_atoms: int) -> tuple:
    """
    Load the dataset and compute the means and standard deviations of energies and forces.
    Args:
        data_prefix (str): The prefix path to the data directory.
        trajectory_dir (str): The directory containing the trajectory data.
        device (torch.device): The device to perform computations on (CPU or GPU).
        fold (int): The fold number for cross-validation.
        num_atoms (int): The number of atoms in the system.
    Returns:
        tuple: A tuple containing the mean and standard deviation of energies and forces.
    """
    train_loader = get_splits_and_load_data(
        data_prefix=data_prefix,
        trajectory_dir=trajectory_dir,
        num_workers=-1,  # Use all available CPU cores
        batch_size=BATCH_SIZE,
        transforms=[],   # No transforms for computing means and stds
        fold=fold
    ).train_dataloader()

    # Compute means and standard deviations of energies and forces
    energy_mean, energy_std, forces_mean, forces_std = compute_means_and_stds(
        device=device,
        train_loader=train_loader,
        num_atoms=num_atoms
    )
    logger.debug(f"Energy mean: {energy_mean.item()} Hartree")
    logger.debug(f"Energy std: {energy_std.item()} Hartree")
    logger.debug(f"Forces mean: {forces_mean.flatten()} Hartree/Å")
    logger.debug(f"Forces std: {forces_std.flatten()} Hartree/Å")

    return energy_mean, energy_std, forces_mean, forces_std

def load_transformed_data_and_compute_stats(data_prefix: str, trajectory_dir: str, device: torch.device, num_atoms: int, 
                                            energy_mean: torch.Tensor, energy_std: torch.Tensor, forces_mean: torch.Tensor, 
                                            forces_std: torch.Tensor) -> tuple:
    """
    Load the dataset with transforms applied and compute the means and standard deviations of energies and forces.
    Args:
        data_prefix (str): The prefix path to the data directory.
        trajectory_dir (str): The directory containing the trajectory data.
        device (torch.device): The device to perform computations on (CPU or GPU).
        num_atoms (int): The number of atoms in the system.
        energy_mean (torch.Tensor): Mean value of energy for standardization.
        energy_std (torch.Tensor): Standard deviation of energy for standardization.
        forces_mean (torch.Tensor): Mean values of forces for standardization.
        forces_std (torch.Tensor): Standard deviations of forces for standardization.
    Returns:
        tuple: A tuple containing the mean and standard deviation of energies and forces after applying transforms.
    """
    # check if the forces mean and std are correct
    transforms = [
        trn.ASENeighborList(cutoff=5.),
        StandardizeEnergy(
            e_key="energy",
            mean_E=energy_mean,
            std_E=energy_std
        ),
        StandardizeForces(
            f_key="forces",
            mean_F=forces_mean,
            std_F=forces_std
        )
    ]
    train_loader = get_splits_and_load_data(
        data_prefix=data_prefix,
        trajectory_dir=trajectory_dir,
        num_workers=-1, # Use all available CPU cores
        batch_size=BATCH_SIZE,
        transforms=transforms
    ).train_dataloader()

    # Initialize accumulators
    standardized_e_mean, standardized_e_std, standardized_f_mean, standardized_f_std = compute_means_and_stds(
        device=device,
        train_loader=train_loader,
        num_atoms=num_atoms
    )

    logger.debug(f"Energy mean: {standardized_e_mean.item()}")
    logger.debug(f"Energy std: {standardized_e_std.item()}")
    logger.debug(f"Forces mean: {standardized_f_mean.flatten()}")
    logger.debug(f"Forces std: {standardized_f_std.flatten()}")

    return standardized_e_mean, standardized_e_std, standardized_f_mean, standardized_f_std

def compute_means_and_stds(device: torch.device, train_loader: pl.LightningDataModule, num_atoms: int) -> tuple:
    """
    Compute the means and standard deviations of energies and forces from the training data.
    Args:
        device (torch.device): The device to perform computations on (CPU or GPU).
        train_loader (pl.LightningDataModule): The data module containing the training data.
        num_atoms (int): The number of atoms in the system.
    Raises:
        ValueError: If the number of atoms in the batch does not match the expected number.
        ValueError: If the force vectors do not have 3 components.
        ValueError: If the energy and forces batch sizes do not match.
    Returns:
        tuple: A tuple containing the mean and standard deviation of energies and forces.
    """
    # Initialize accumulators for means and variances
    energy_mean = torch.zeros((), dtype=torch.float64, device=device)
    energy_M2   = torch.zeros_like(energy_mean)
    forces_mean = torch.zeros((num_atoms,3), dtype=torch.float64, device=device)
    forces_M2   = torch.zeros_like(forces_mean)
    count = 0

    # Single‐pass Welford (see wikipedia for details)
    for batch in tqdm(train_loader, desc="Computing stats"):
        e_batch = batch["energy"].to(dtype=torch.float64, device=device)   # (B,)
        f_batch = batch["forces"].to(dtype=torch.float64, device=device)   # (B,N,3)

        if f_batch.shape[0] != num_atoms:
            raise ValueError(f"Number of atoms in batch {f_batch.shape[0]} does not match expected number {num_atoms}.")
        if f_batch.shape[1] != 3:
            raise ValueError(f"Force vectors should have 3 components, but got {f_batch.shape[2]}.")

        for i in range(e_batch.shape[0]):
            # increment count and do energy and forces updates
            count += 1
            energy_mean, energy_M2 = welford_update(e_batch[i], energy_mean, energy_M2, count)
            forces_mean, forces_M2 = welford_update(f_batch[i], forces_mean, forces_M2, count)

    # Finalize
    energy_var  = energy_M2 / count
    energy_std  = torch.sqrt(energy_var)

    forces_var  = forces_M2 / count
    forces_std  = torch.sqrt(forces_var)
    return energy_mean, energy_std, forces_mean, forces_std

def welford_update(x, mean, M2, count) -> tuple:
    """
    Perform a Welford update for mean and variance.
    Args:
        x (torch.Tensor): The new data point to update the mean and variance with.
        mean (torch.Tensor): The current mean value.
        M2 (torch.Tensor): The current second moment (sum of squares).
        count (int): The current count of data points.
    Returns:
        tuple: Updated mean and second moment (M2).
    """
    delta = x - mean
    mean += delta / count
    delta2 = x - mean
    M2   += delta * delta2
    return mean, M2

def check_means_and_stds(energy_mean: torch.Tensor, energy_std: torch.Tensor, forces_mean: torch.Tensor, forces_std: torch.Tensor, num_atoms: int, device: torch.device):
    """
    Check if the computed means and standard deviations are as expected.
    Args:
        energy_mean (torch.Tensor): Mean of energies.
        energy_std (torch.Tensor): Standard deviation of energies.
        forces_mean (torch.Tensor): Mean of forces.
        forces_std (torch.Tensor): Standard deviation of forces.
        num_atoms (int): Number of atoms in the system.
        device (torch.device): The device to perform computations on (CPU or GPU).
    Raises:
        AssertionError: If the means and standard deviations do not match the expected values.
    """
    assert torch.isclose(energy_mean, torch.tensor(0.0, dtype=torch.float64, device=device)), f"Energy mean ≠ 0: {energy_mean:.6f}"
    assert torch.isclose(energy_std, torch.tensor(1.0, dtype=torch.float64, device=device)), f"Energy std ≠ 1: {energy_std:.6f}"
    assert torch.allclose(forces_mean, torch.zeros((num_atoms, 3), dtype=torch.float64, device=device)), f"Forces mean ≠ 0: {forces_mean.abs().max():.3e}"
    assert torch.allclose(forces_std, torch.ones((num_atoms, 3), dtype=torch.float64, device=device)), f"Forces std ≠ 1: {forces_std.abs().max():.3e}"

def save_means_and_stds(save_path: str, energy_mean: torch.Tensor, energy_std: torch.Tensor, forces_mean: torch.Tensor, forces_std: torch.Tensor):
    """
    Save the computed means and standard deviations to a file.
    Args:
        save_path (str): The path to the file where means and standard deviations will be saved.
        energy_mean (torch.Tensor): Mean of energies.
        energy_std (torch.Tensor): Standard deviation of energies.
        forces_mean (torch.Tensor): Mean of forces.
        forces_std (torch.Tensor): Standard deviation of forces.
    """
    
    torch.save({
        "energy_mean": energy_mean,
        "energy_std": energy_std,
        "forces_mean": forces_mean,
        "forces_std": forces_std
    }, save_path)
    logger.info(f"Means and standard deviations saved to {save_path}")

def main(trajectory_dir: str, fold: int, num_atoms: int):
    # Setup 
    data_prefix = set_data_prefix()
    # SchNetPack seems to load data not only with GPU, but also with CPU, so we set the device to CPU
    device = torch.device("cpu")
    means_stds_path = os.path.join(data_prefix, trajectory_dir, f"means_stds_fold_{fold}.pt")
    
    if not os.path.exists(means_stds_path):
        logger.info(f"Means and standard deviations file does not exist, computing them")
        energy_mean, energy_std, forces_mean, forces_std = load_data_and_compute_stats(
            data_prefix=data_prefix,
            trajectory_dir=trajectory_dir,
            device=device,
            fold=fold,
            num_atoms=num_atoms
        )
    else:
        logger.info(f"Means and standard deviations file exists, loading them")
        means_stds = torch.load(means_stds_path, map_location=device, weights_only=True)
        energy_mean = means_stds["energy_mean"]
        energy_std = means_stds["energy_std"]
        forces_mean = means_stds["forces_mean"]
        forces_std = means_stds["forces_std"]

    standardized_e_mean, standardized_e_std, standardized_f_mean, standardized_f_std = load_transformed_data_and_compute_stats(
        data_prefix=data_prefix,
        trajectory_dir=trajectory_dir,
        device=device,
        num_atoms=num_atoms,
        energy_mean=energy_mean,
        energy_std=energy_std,
        forces_mean=forces_mean,
        forces_std=forces_std
    )

    # check if means are zero and stds are one
    check_means_and_stds(standardized_e_mean, standardized_e_std, standardized_f_mean, standardized_f_std, num_atoms, device)

    if not os.path.exists(means_stds_path):
        # Save the computed means and stds to a file
        save_means_and_stds(
            save_path=means_stds_path,
            energy_mean=energy_mean,
            energy_std=energy_std,
            forces_mean=forces_mean,
            forces_std=forces_std
        )

if __name__ == "__main__":
    args = parse_args()
    main(**args)