import os
import pytorch_lightning as pl
import argparse

from hydra import initialize, compose
from hydra.utils import instantiate, get_class
from omegaconf import OmegaConf, DictConfig


from md_with_schnet.utils import setup_logger, load_xtb_dataset, set_data_prefix

logger = setup_logger("info")

# Example command to run the script from within code directory:
"""
screen -dmS xtb_train sh -c 'python -m md_with_schnet.neural_net.train --trajectory_dir MOTOR_MD_XTB/T300_1 -bs 100 -e 2 ; exec bash'
"""

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for training SchNetPack on XTB datasets.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="MOTOR_MD_XTB/T300_1", help="Directory containing the trajectory data generated by Turbomole (default: MOTOR_MD_XTB/T300_1)")
    # training setup
    parser.add_argument("-bs", "--batch_size", type=int, default=100, help="Batch size for training (default: 100)")
    parser.add_argument("-e", "--num_epochs", type=int, default=1, help="Number of epochs for training (default: 1)")
    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-4, help="Learning rate for the optimizer (default: 1e-4)")
    parser.add_argument("-nw", "--num_workers", type=int, default=-1, help="Number of workers for data loading (default: -1, which sets it to 0 on macOS and 31 on Linux)")
    return vars(parser.parse_args())

def update_config(cfg: DictConfig, run_path: str, batch_size: int, num_epochs: int, learning_rate: float, num_workers: int) -> DictConfig:
    """ Update the configuration with command-line arguments.
    
    Args:
        cfg (DictConfig): The original configuration.
        run_path (str): Path to save the run.
        batch_size (int): Batch size for training.
        num_epochs (int): Number of epochs for training.
        learning_rate (float): Learning rate for the optimizer.
        num_workers (int): Number of workers for data loading.
    
    Returns:
        DictConfig: Updated configuration.
    """
    cfg.run.path = run_path
    cfg.data.batch_size = batch_size
    if num_workers != -1:
        cfg.data.num_workers = num_workers
    cfg.trainer.max_epochs = num_epochs
    cfg.globals.lr = learning_rate
    return cfg

def main(trajectory_dir: str, batch_size: int, num_epochs: int, learning_rate: float, num_workers: int):
    ####################### 1) Compose the config ###########################
    with initialize(config_path="conf", job_name="train"):
        cfg: DictConfig = compose(config_name="train_config")

    # set run path
    current_dir = os.path.dirname(os.path.abspath(__file__))
    run_name = f"{trajectory_dir}_epochs_{num_epochs}_bs_{batch_size}_lr_{learning_rate}"
    run_name = run_name.replace("/", "_")
    run_path = os.path.join(current_dir, "runs", run_name)
    

    # update the config with the arguments from the command line
    cfg = update_config(cfg, run_path, batch_size, num_epochs, learning_rate, num_workers)
    logger.info(f"Loaded and updated config:\n{OmegaConf.to_yaml(cfg)}")

    ####################### 2) Switch strings to classes ####################
    optim_cls = get_class(cfg.task.optimizer_cls)  
    sched_cls = get_class(cfg.task.scheduler_cls)

    ####################### 3) Prepare our own data #########################
    data_prefix = set_data_prefix()
    output_dir = os.path.join(data_prefix, "output")
    os.makedirs(output_dir, exist_ok=True)

    splits_dir = os.path.join(data_prefix, "splits", trajectory_dir)
    split_file = os.path.join(splits_dir, "inner_splits_0.npz")
    if not os.path.exists(split_file):
        raise FileNotFoundError(f"Missing split file: {split_file}")
    path_to_db = os.path.join(data_prefix, trajectory_dir, "md_trajectory.db")

    datamodule = load_xtb_dataset(path_to_db, cfg, split_file)

    ####################### 4) Instantiate model & task from YAML ###########
    model = instantiate(cfg.model)
    task = instantiate(
        cfg.task,
        model=model,
        optimizer_cls=optim_cls,
        scheduler_cls=sched_cls
    )

    ####################### 5) Logger ################################
    hparams = OmegaConf.to_container(cfg, resolve=False)
    # Remove non‐primitive entries
    hparams["task"].pop("outputs", None)

    tb_logger = instantiate(cfg.logger.tensorboard)
    # Manually log your clean hyperparameters
    tb_logger.log_hyperparams(hparams)

    # Prevent Lightning from auto‐saving hparams (no‐op override)
    tb_logger.log_hyperparams = lambda *args, **kwargs: None


    ####################### 6) Callbacks ##############################
    callbacks = [
        instantiate(cfg.callbacks.model_checkpoint),
        instantiate(cfg.callbacks.early_stopping),
        instantiate(cfg.callbacks.lr_monitor),
        instantiate(cfg.callbacks.ema),
    ]

    ####################### 7) Trainer ################################
    trainer: pl.Trainer = instantiate(
        cfg.trainer,
        callbacks=callbacks,
        logger=tb_logger,
        default_root_dir=cfg.run.path,
    )
    print(f"cfg.run.ckpt_path: {cfg.run.ckpt_path}")
    print(f"cfg.run.path: {cfg.run.path}")

    ####################### 8) Launch training ########################
    trainer.fit(task, datamodule=datamodule)

if __name__ == "__main__":
    args = parse_args()
    main(**args)
