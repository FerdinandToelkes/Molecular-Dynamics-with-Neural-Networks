import os
import pytorch_lightning as pl
import argparse
import platform
import time

from hydra import initialize, compose
from hydra.utils import instantiate, get_class
from omegaconf import OmegaConf, DictConfig

# own imports
from md_with_schnet.utils import setup_logger, load_xtb_dataset, set_data_prefix

logger = setup_logger("debug")

# Example command to run the script from within code directory:
"""
screen -dmS xtb_train sh -c 'python -m md_with_schnet.neural_net.train --trajectory_dir MOTOR_MD_XTB/T300_1 -e 1000 ; exec bash'
"""

def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for training SchNetPack on XTB datasets.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="MOTOR_MD_XTB/T300_1", help="Directory containing the trajectory data generated by Turbomole (relative to the data directory) (default: MOTOR_MD_XTB/T300_1)")
    # training setup
    parser.add_argument("-bs", "--batch_size", type=int, default=100, help="Batch size for training (default: 100)")
    parser.add_argument("-e", "--num_epochs", type=int, default=1, help="Number of epochs for training (default: 1)")
    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-4, help="Learning rate for the optimizer (default: 1e-4)")
    parser.add_argument("-nw", "--num_workers", type=int, default=-1, help="Number of workers for data loading (default: -1, which sets it to 0 on macOS and 31 on Linux)")
    # others
    parser.add_argument("-s", "--seed", type=int, default=42, help="Random seed for reproducibility (default: 42)")
    return vars(parser.parse_args())

def set_run_path(trajectory_dir: str, num_epochs: int, batch_size: int, learning_rate: float, seed: int) -> str:
    """ 
    Set the run path based on the trajectory directory.
    Args:
        trajectory_dir (str): Directory containing the trajectory data.
        num_epochs (int): Number of epochs for training.
        batch_size (int): Batch size for training.
        learning_rate (float): Learning rate for the optimizer.
        seed (int): Random seed for reproducibility.
    Returns:
        str: The run path.
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    run_name = f"{trajectory_dir}_epochs_{num_epochs}_bs_{batch_size}_lr_{learning_rate}_seed_{seed}"
    run_name = run_name.replace("/", "_")
    run_path = os.path.join(current_dir, "runs", run_name)
    return run_path

def update_config(cfg: DictConfig, run_path: str, batch_size: int, num_epochs: int, learning_rate: float, num_workers: int) -> DictConfig:
    """ 
    Update the configuration with command-line arguments.
    Args:
        cfg (DictConfig): The original configuration.
        run_path (str): Path to save the run.
        batch_size (int): Batch size for training.
        num_epochs (int): Number of epochs for training.
        learning_rate (float): Learning rate for the optimizer.
        num_workers (int): Number of workers for data loading.
    Returns:
        DictConfig: Updated configuration.
    """
    cfg.run.path = run_path
    cfg.data.batch_size = batch_size
    if num_workers != -1:
        cfg.data.num_workers = num_workers
    else:
        cfg.data.num_workers = 0 if platform.system() == 'Darwin' else 31
    cfg.trainer.max_epochs = num_epochs
    cfg.globals.lr = learning_rate
    return cfg

def get_split_path(data_prefix: str, trajectory_dir: str, fold: int = 0) -> str:
    """
    Get the path to the split file for the given trajectory directory and fold.
    Args:
        data_prefix (str): The prefix path to the data directory.
        trajectory_dir (str): The directory containing the trajectory data.
        fold (int): The fold number for cross-validation (default: 0).
    Returns:
        str: The path to the split file.
    """
    split_file = os.path.join(data_prefix, "splits", trajectory_dir, f"inner_splits_{fold}.npz")
    if not os.path.exists(split_file):
        raise FileNotFoundError(f"Missing split file: {split_file}")
    logger.debug(f"Split file: {split_file}")
    return split_file

def prepare_and_load_data(data_prefix: str, cfg: DictConfig, trajectory_dir: str) -> pl.LightningDataModule:
    """
    Prepare loading the dataset and then load it.
    Args:
        data_prefix (str): The prefix path to the data directory.
        cfg (DictConfig): The configuration dictionary.
        trajectory_dir (str): The directory containing the trajectory data.
    Returns:
        pl.LightningDataModule: The data module containing the dataset.
    """
    split_file = get_split_path(data_prefix, trajectory_dir, fold=0)

    if not os.path.exists(split_file):
        raise FileNotFoundError(f"Split file does not exist: {split_file}, try running the create_splits.py script first.")

    path_to_db = os.path.join(data_prefix, trajectory_dir, "md_trajectory.db")
    logger.debug(f"Path to database: {path_to_db}")

    datamodule = load_xtb_dataset(path_to_db, cfg, split_file)
    return datamodule

def main(trajectory_dir: str, batch_size: int, num_epochs: int, learning_rate: float, num_workers: int, seed: int):
    ####################### Basic setup ###########################
    pl.seed_everything(seed, workers=True)

    ####################### 1) Compose the config ###########################
    with initialize(config_path="conf", job_name="train", version_base="1.1"):
        cfg: DictConfig = compose(config_name="train_config")

    # set run path
    run_path = set_run_path(trajectory_dir, num_epochs, batch_size, learning_rate, seed)
    logger.debug(f"Run path: {run_path}")
    
    # update the config with the arguments from the command line
    cfg = update_config(cfg, run_path, batch_size, num_epochs, learning_rate, num_workers)
    logger.info(f"Loaded and updated config:\n{OmegaConf.to_yaml(cfg)}")

    ####################### 2) Switch strings to classes ####################
    optim_cls = get_class(cfg.task.optimizer_cls)  
    sched_cls = get_class(cfg.task.scheduler_cls)

    ####################### 3) Prepare our own data #########################
    data_prefix = set_data_prefix()

    datamodule = prepare_and_load_data(data_prefix, cfg, trajectory_dir)

    ####################### 4) Instantiate model & task from YAML ###########
    model: pl.LightningModule = instantiate(cfg.model)
    task: pl.LightningModule = instantiate(
        cfg.task,
        model=model,
        optimizer_cls=optim_cls,
        scheduler_cls=sched_cls
    )


    ####################### 5) Logger ################################
    # Convert the config to a basic dict with resolved values, i.e. ${work_dir} -> /path/to/work_dir
    used_config = OmegaConf.to_container(cfg, resolve=True)

    # instantiate the logger
    tb_logger = instantiate(cfg.logger.tensorboard)

    # Manually log your clean hyperparameters
    tb_logger.log_hyperparams(used_config) # saves as hparams.yaml (difficult to change)

    # Prevent Lightning from auto‐saving used_config (no‐op override)
    # use lambda function taking any number of arguments and returning None
    tb_logger.log_hyperparams = lambda *args, **kwargs: None


    ####################### 6) Callbacks ##############################
    callbacks: list = [
        instantiate(cfg.callbacks.model_checkpoint),
        instantiate(cfg.callbacks.early_stopping),
        instantiate(cfg.callbacks.lr_monitor),
        instantiate(cfg.callbacks.ema),
    ]

    ####################### 7) Trainer ################################
    trainer: pl.Trainer = instantiate(
        cfg.trainer,
        callbacks=callbacks,
        logger=tb_logger,
        deterministic=True, # for reproducibility
    )
    

    ####################### 8) Launch training ########################
    # since we have a pl datamodule, it is split automatically into train, val and test sets
    # depending on which method is called, e.g. fit, validate, test
    # see https://lightning.ai/docs/pytorch/stable/data/datamodule.html for more details
    trainer.fit(task, datamodule=datamodule)

if __name__ == "__main__":
    args = parse_args()
    start = time.time()
    main(**args)
    print(f"Total time: {time.time() - start:.2f} seconds")


# TIMINGS 5 EPOCHS (on pc38)
# MOTOR_MD_XTB/T300_1 with roughly 13000 training samples

# Batch size: 100
# num_workers: 0  - 139.43s
# num_workers: 15 - 18.58s
# num_workers: 31 - 17.40s

# Batch size: 10
# num_workers: 0  - 191.98s
# num_workers: 15 - 48.62s
# num_workers: 31 - 50.66s (and led to errors sometimes)

# Now its somehow much slower, even with 31 workers

# TIMING 1000 EPOCHS (on pc54)
# MOTOR_MD_XTB/T300_1 with 12615 training samples, bs=100, num_workers=31
# Total time: 20825.10 seconds = 5.78 hours