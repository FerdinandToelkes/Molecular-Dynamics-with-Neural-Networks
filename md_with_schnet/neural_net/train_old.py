import torch
import torchmetrics
import schnetpack as spk
import schnetpack.transform as trn
import pytorch_lightning as pl
import os
import argparse
import time

from hydra import initialize, compose
from omegaconf import OmegaConf

from md_with_schnet.utils import setup_logger, load_xtb_dataset, set_data_prefix


logger = setup_logger("info")

# Example command to run the script from within code directory:
"""
python -m md_with_schnet.neural_net.train --trajectory_dir MOTOR_MD_XTB/T300_1 -bs 100 -e 1 -lr 1e-4
"""

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for training SchNetPack on XTB datasets.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="MOTOR_MD_XTB/T300_1", help="Directory containing the trajectory data generated by Turbomole (default: MOTOR_MD_XTB/T300_1)")
    # training setup
    parser.add_argument("-bs", "--batch_size", type=int, default=100, help="Batch size for training (default: 100)")
    parser.add_argument("-e", "--num_epochs", type=int, default=1, help="Number of epochs for training (default: 1)")
    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-4, help="Learning rate for the optimizer (default: 1e-4)")
    parser.add_argument("-nw", "--num_workers", type=int, default=-1, help="Number of workers for data loading (default: -1, which sets it to 0 on macOS and 31 on Linux)")
    return vars(parser.parse_args())



def main(trajectory_dir: str, batch_size: int, num_epochs: int, learning_rate: float, num_workers: int):
    """ Main function to train SchNetPack on XTB datasets.
    Args:
        trajectory_dir (str): Directory containing the trajectory data generated by Turbomole.
        batch_size (int): Batch size for training.
        num_epochs (int): Number of epochs for training.
        learning_rate (float): Learning rate for the optimizer.
    """
    # setup
    data_prefix = set_data_prefix()
    output_dir = os.path.join(data_prefix, 'output')
    if not os.path.exists(output_dir):
        logger.info(f"Creating output directory: {output_dir}")
        os.makedirs(output_dir)
    # Initialize Hydra and compose the configuration
    with initialize(config_path="."):
        cfg = compose(config_name="reference_config.yaml")
        logger.info(f"Configuration: {OmegaConf.to_yaml(cfg)}")

    splits_dir = os.path.join(data_prefix, 'splits', trajectory_dir)
    split_file = os.path.join(splits_dir, "inner_splits_0.npz")
    if not os.path.exists(split_file):
        logger.error(f"Split file not found: {split_file}. Please run the create_splits.py script first.")
        raise FileNotFoundError(f"Split file not found: {split_file}. Please run the create_splits.py script first.")
    path_to_data = os.path.join(data_prefix, trajectory_dir, 'md_trajectory.db')

    # Load our XTB dataset using parameters from the config
    data = load_xtb_dataset(path_to_data, cfg, split_file)

    # print some information about the dataset
    properties = data.dataset[0]
    properties_str = ''.join(f'{i}\n' for i in properties.keys())
    logger.info(f"Loaded properties:\n{properties_str}")
    logger.info(f'Shape:\n{properties["forces"].shape}')
    
    

    # BUILD MODEL
    # Define representation
    cutoff = cfg.globals.cutoff
    n_atom_basis = cfg.model.representation.n_atom_basis

    pairwise_distance = spk.atomistic.PairwiseDistances() # calculates pairwise distances between atoms
    radial_basis = spk.nn.GaussianRBF(
        n_rbf=cfg.model.representation.radial_basis.n_rbf, 
        cutoff=cutoff)
    schnet = spk.representation.SchNet(
        n_atom_basis=n_atom_basis, 
        n_interactions=cfg.model.representation.n_interactions,
        radial_basis=radial_basis,
        cutoff_fn=spk.nn.CosineCutoff(cutoff)
    )
    exit()
    # Define output modules
    pred_energy = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key="energy")
    pred_forces = spk.atomistic.Forces(energy_key="energy", force_key="forces")

    # Combine into a model
    nnpot = spk.model.NeuralNetworkPotential(
        representation=schnet, # TODO: try different representations such as ACSF
        input_modules=[pairwise_distance],
        output_modules=[pred_energy, pred_forces],
        postprocessors=[
            trn.CastTo64(),
            trn.AddOffsets("energy", add_mean=True, add_atomrefs=False)
        ]
    )

    # Define loss function
    output_energy = spk.task.ModelOutput(
        name="energy",
        loss_fn=torch.nn.MSELoss(),
        loss_weight=cfg.task.outputs[0].loss_weight,
        metrics={"MAE": torchmetrics.MeanAbsoluteError()}
    )

    output_forces = spk.task.ModelOutput(
        name="forces",
        loss_fn=torch.nn.MSELoss(),
        loss_weight=cfg.task.outputs[1].loss_weight,
        metrics={"MAE": torchmetrics.MeanAbsoluteError()}
    )

    # Define task
    task = spk.task.AtomisticTask(
        model=nnpot,
        outputs=[output_energy, output_forces],
        optimizer_cls=torch.optim.AdamW,
        optimizer_args={"lr": learning_rate}
    )

    # set logger and callbacks
    train_logger = pl.loggers.TensorBoardLogger(save_dir=output_dir)
    callbacks = [
        spk.train.ModelCheckpoint(
            model_path=os.path.join(output_dir, "best_inference_model"),
            save_top_k=1,
            monitor="val_loss"
        )
    ]

    # set up the trainer and fit the model
    trainer = pl.Trainer(
        callbacks=callbacks,
        logger=train_logger,
        default_root_dir=output_dir,
        max_epochs=num_epochs, 
    )
    trainer.fit(task, datamodule=data)


if __name__ == "__main__":
    args = parse_args()
    start = time.time()
    main(**args)
    logger.info(f"Total time taken: {time.time() - start:.2f} seconds")




# TIMINGS 5 EPOCHS 
# MOTOR_MD_XTB/T300_1 with roughly 13000 training samples

# Batch size: 100
# num_workers: 0  - 139.43s
# num_workers: 15 - 18.58s
# num_workers: 31 - 17.40s

# Batch size: 10
# num_workers: 0  - 191.98s
# num_workers: 15 - 48.62s
# num_workers: 31 - 50.66s (and led to errors sometimes)
