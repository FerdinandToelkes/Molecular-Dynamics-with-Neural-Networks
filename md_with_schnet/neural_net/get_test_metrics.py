import os
import argparse
import platform
import torch
import pytorch_lightning as pl
import numpy as np

from ase import Atoms
from xtb_ase.calculator import XTB
from omegaconf import OmegaConf, DictConfig
from schnetpack.utils.compatibility import load_model
from tqdm import tqdm
from ase import units

from md_with_schnet.utils import set_data_prefix, get_split_path, load_config, setup_datamodule
from md_with_schnet.setup_logger import setup_logger

BOHR_TO_ANGSTROM = units.Bohr  # 1 Bohr = 0.52917721067 Ã…
HARTREE_TO_EV = units.Hartree  # 1 eV = 27.21138602 Hartree
BATCH_SIZE = 1 # needed due to weird batch shape of schnetpack data module

# Example command to run the script from within code directory:
"""
screen -dmS get_test_metrics sh -c 'python -m md_with_schnet.neural_net.get_test_metrics -mdir MOTOR_MD_XTB_T300_1_epochs_1000_bs_100_lr_0.0001_seed_42_default_trafos ; exec bash'
"""


logger = setup_logger("debug")

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for predicting with trained model on XTB test data.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="MOTOR_MD_XTB/T300_1", help="Directory containing the trajectory data generated by Turbomole (default: MOTOR_MD_XTB/T300_1)")
    parser.add_argument("-mdir", "--model_dir", type=str, default="MOTOR_MD_XTB_T300_1_epochs_1000_bs_100_lr_0.0001_seed_42", help="Directory of the trained model (default: MOTOR_MD_XTB_T300_1_epochs_1000_bs_100_lr_0.0001_seed_42)")
    parser.add_argument("-f", "--fold", type=int, default=0, help="Fold number for cross-validation (default: 0)")
    parser.add_argument("-s", "--seed", type=int, default=42, help="Random seed for reproducibility (default: 42)")
    parser.add_argument("-nw", "--num_workers", type=int, default=-1, help="Number of workers for data loading (default: -1, which sets it to 0 on macOS and 8 on Linux)")
    return vars(parser.parse_args())



def update_config_with_train_config(cfg: DictConfig, cfg_train: DictConfig) -> DictConfig:
    """
    Update the configuration with the training configuration to ensure consistency.
    Args:
        cfg (DictConfig): The original configuration.
        cfg_train (DictConfig): The training configuration.
    Returns:
        DictConfig: Updated configuration.
    """
    # Disable struct mode to allow adding new fields
    OmegaConf.set_struct(cfg, False)

    # Copy over specific globals and of run
    cfg.globals.cutoff = cfg_train.globals.cutoff
    cfg.globals.energy_key = cfg_train.globals.energy_key
    cfg.globals.forces_key = cfg_train.globals.forces_key
    cfg.run.mean_std_path = cfg_train.run.mean_std_path

    # Copy over data fields
    cfg.data = cfg_train.data
    # Create model field if it doesn't exist
    if 'model' not in cfg:
        cfg.model = DictConfig({})
    # Copy over model fields
    cfg.model.postprocessors = cfg_train.model.postprocessors

    # Re-enable struct mode to enforce the structure
    OmegaConf.set_struct(cfg, True)
    return cfg

def update_config(cfg: DictConfig, run_path: str, num_workers: int) -> DictConfig:
    """ 
    Update the configuration with command-line arguments.
    Args:
        cfg (DictConfig): The original configuration.
        run_path (str): Path to save the run.
        num_workers (int): Number of workers for data loading.
    Returns:
        DictConfig: Updated configuration.
    """
    cfg.run.path = run_path
    if num_workers != -1:
        cfg.data.num_workers = num_workers
    else:
        cfg.data.num_workers = 0 if platform.system() == 'Darwin' else 8
    cfg.org_data.batch_size = BATCH_SIZE  # Set batch size for inference
    return cfg

def compute_metrics_for_neural_net(cfg: DictConfig, datamodule: pl.LightningDataModule, model: torch.nn.Module):
    """ Compute metrics for the neural network model on the test data.
    Args:
        cfg (DictConfig): Configuration containing model and data parameters.
        nn_datamodule: Data module containing the test data.
        model: The trained neural network model.
    Returns:
        tuple: Mean Absolute Error (MAE) for forces and energy.
    """
    forces_mae = energy_mae = 0
    for batch in tqdm(datamodule.test_dataloader(), desc="Evaluating model on test data"): 
        e_gt = batch[cfg.globals.energy_key].numpy()
        f_gt = batch[cfg.globals.forces_key].numpy()
        # forward pass
        output = model(batch)
        # enegry and forces required grads -> detach them
        e_pred = output[cfg.globals.energy_key].detach().numpy()
        f_pred = output[cfg.globals.forces_key].detach().numpy()
        # compute metrics
        forces_mae += get_batch_mae(f_pred, f_gt)
        energy_mae += get_batch_mae(e_pred, e_gt)
 
    forces_mae, energy_mae = average_over_batches(datamodule, forces_mae, energy_mae)

    return forces_mae, energy_mae

def compute_metrics_for_xtb(cfg: DictConfig, datamodule: pl.LightningDataModule):
    """ Compute metrics for the XTB model on the test data.
    Args:
        cfg (DictConfig): Configuration containing model and data parameters.
        datamodule: Data module containing the test data.
    Returns:
        tuple: Mean Absolute Error (MAE) for forces and energy.
    """
    forces_mae = energy_mae = 0
    for batch in tqdm(datamodule.test_dataloader(), desc="Evaluating model on test data"):
        nums = batch["_atomic_numbers"].numpy()
        pos = batch["_positions"].numpy()   
        e_gt = batch[cfg.globals.energy_key].numpy() 
        f_gt = batch[cfg.globals.forces_key].numpy()

        # convert positions from Bohr to Angstrom and
        pos = pos * BOHR_TO_ANGSTROM  # convert positions to Angstrom

        atoms = Atoms(numbers=nums, positions=pos)
        atoms.calc = XTB(method=cfg.xtb.method)
        e_pred = atoms.get_potential_energy() # eV
        f_pred = atoms.get_forces() # eV/Angstrom

        # convert predicted properties to atomic units
        e_pred = e_pred / HARTREE_TO_EV  # Hartree
        f_pred = f_pred * (BOHR_TO_ANGSTROM / HARTREE_TO_EV) # Hartree/Bohr
            
        # compute metrics
        forces_mae += get_batch_mae(f_pred, f_gt)
        energy_mae += get_batch_mae(e_pred, e_gt)

    forces_mae, energy_mae = average_over_batches(datamodule, forces_mae, energy_mae)

    return forces_mae, energy_mae

def average_over_batches(datamodule: pl.LightningDataModule, forces_mae: float, energy_mae: float):
    """ 
    Average the computed metrics over all batches.
    Args:
        datamodule: Data module containing the test data.
        forces_mae (float): Mean Absolute Error for forces.
        energy_mae (float): Mean Absolute Error for energy.
    Returns:
        tuple: Averaged Mean Absolute Error (MAE) for forces and energy.
    """
    num_batches = len(datamodule.test_dataloader())
    return forces_mae / num_batches, energy_mae / num_batches

def get_batch_mae(pred: np.ndarray, gt: np.ndarray) -> float:
    """ Compute Mean Absolute Error (MAE) for a batch.
    Args:
        pred (np.ndarray): Predicted values.
        gt (np.ndarray): Ground truth values.
    Returns:
        float: Mean Absolute Error for the batch.
    """
    return np.mean(np.abs(pred - gt)).item()

def log_final_metrics(nn_forces_mae: float, nn_energy_mae: float, xtb_forces_mae: float, xtb_energy_mae: float):
    """ Log the final metrics for both neural network and XTB models.
    Args:
        nn_forces_mae (float): Mean Absolute Error for forces from the neural network.
        nn_energy_mae (float): Mean Absolute Error for energy from the neural network.
        xtb_forces_mae (float): Mean Absolute Error for forces from the XTB model.
        xtb_energy_mae (float): Mean Absolute Error for energy from the XTB model.
    """
    logger.info(f"Test metrics for nn:")
    logger.info(f"Forces MAE: {nn_forces_mae:.6f} Hartree/Bohr")
    logger.info(f"Energy MAE: {nn_energy_mae:.6f} Hartree")
    logger.info(f"Test metrics for XTB:")
    logger.info(f"Forces MAE: {xtb_forces_mae:.6f} Hartree/Bohr")
    logger.info(f"Energy MAE: {xtb_energy_mae:.6f} Hartree")

def main(trajectory_dir: str, model_dir: str, fold: int, seed: int, num_workers: int):
    pl.seed_everything(seed, workers=True)

    ####################### 1) Compose the config ###########################
    cfg = load_config("neural_net/conf", "inference_config", "inference")

    # use training config to update the inference config
    train_cfg_path = os.path.join("neural_net/runs", model_dir, cfg.globals.train_config_subpath)
    cfg_train = load_config(train_cfg_path, cfg.globals.hparams_file_name, "train")
    cfg = update_config_with_train_config(cfg, cfg_train)

    # update config with arguments from command line
    home_dir = os.path.expanduser("~")
    runs_dir_path = os.path.join(home_dir, cfg.globals.runs_dir_subpath)
    model_dir_path = os.path.join(runs_dir_path, model_dir)
    cfg = update_config(cfg, model_dir_path, num_workers)
    logger.info(f"Loaded and updated config:\n{OmegaConf.to_yaml(cfg)}")

    ####################### 2) Prepare Data and Paths #########################
    data_prefix = set_data_prefix()
    split_file = get_split_path(data_prefix, trajectory_dir, fold)
    path_to_db = os.path.join(data_prefix, trajectory_dir, "md_trajectory.db")
    
    datamodule = setup_datamodule(data_cfg=cfg.org_data, datapath=path_to_db, split_file=split_file)
    
    ####################### 3) Evaluate model on test data ##############################    
    model = load_model(cfg.globals.model_path, device="cpu")
    model.eval()

    nn_forces_mae, nn_energy_mae = compute_metrics_for_neural_net(cfg, datamodule, model)
    xtb_forces_mae, xtb_energy_mae = None, None
    #xtb_forces_mae, xtb_energy_mae = compute_metrics_for_xtb(cfg, datamodule)
    
    log_final_metrics(nn_forces_mae, nn_energy_mae, xtb_forces_mae, xtb_energy_mae)


if __name__ == "__main__":
    args = parse_args()
    main(**args)

