import os
import pytorch_lightning as pl
import argparse
import importlib
import schnetpack as spk

from hydra import initialize, compose
from hydra.utils import instantiate
from omegaconf import OmegaConf, DictConfig


from md_with_schnet.utils import setup_logger, load_xtb_dataset, set_data_prefix

logger = setup_logger("info")

# Example command to run the script from within code directory:
"""
python -m md_with_schnet.neural_net.train_2 --trajectory_dir MOTOR_MD_XTB/T300_1 -bs 100 -e 1 -lr 1e-4
"""

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for training SchNetPack on XTB datasets.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="MOTOR_MD_XTB/T300_1", help="Directory containing the trajectory data generated by Turbomole (default: MOTOR_MD_XTB/T300_1)")
    return vars(parser.parse_args())

def _str_to_class(path: str):
    """Given 'package.module.ClassName', return the actual ClassName object."""
    module, cls = path.rsplit(".", 1)
    return getattr(importlib.import_module(module), cls)

def main(trajectory_dir: str):

    # ─── 1) Compose the config ───────────────────────────────
    with initialize(config_path="conf", job_name="train"):
        cfg: DictConfig = compose(config_name="my_config")
    logger.info("Loaded config:\n" + OmegaConf.to_yaml(cfg))

    # ─── 2) Patch any bare‐string class paths into real classes ─
    #   (only needed if you used string literals instead of _target_)
    if isinstance(cfg.task.optimizer_cls, str):
        cfg.task.optimizer_cls = _str_to_class(cfg.task.optimizer_cls)
    if isinstance(cfg.task.scheduler_cls, str):
        cfg.task.scheduler_cls = _str_to_class(cfg.task.scheduler_cls)

    # ─── 3) Prepare DataModule ────────────────────────────────
    data_prefix = set_data_prefix()
    output_dir = os.path.join(data_prefix, "output")
    os.makedirs(output_dir, exist_ok=True)

    splits_dir = os.path.join(data_prefix, "splits", trajectory_dir)
    split_file = os.path.join(splits_dir, "inner_splits_0.npz")
    if not os.path.exists(split_file):
        raise FileNotFoundError(f"Missing split file: {split_file}")
    path_to_db = os.path.join(data_prefix, trajectory_dir, "md_trajectory.db")

    datamodule = load_xtb_dataset(
        path_to_db,
        batch_size=cfg.data.batch_size,
        split_file=split_file,
        num_workers=cfg.data.num_workers,
    )

    # ─── 4) Instantiate model from YAML ────────────────────────
    model = instantiate(cfg.model)

    # ─── 5) Build AtomisticTask by converting config → dict ────
    # Resolve all interpolations and turn into a pure-Python dict
    task_dict = OmegaConf.to_container(cfg.task, resolve=True)
    # Now inject our already-instantiated model
    task = spk.task.AtomisticTask(model=model, **task_dict)

    # ─── 6) Callbacks & Logger ───────────────────────────────
    callbacks = [
        instantiate(cfg.callbacks.model_checkpoint),
        instantiate(cfg.callbacks.early_stopping),
        instantiate(cfg.callbacks.lr_monitor),
        instantiate(cfg.callbacks.ema),
    ]
    tb_logger = instantiate(cfg.logger.tensorboard)

    # ─── 7) Trainer ───────────────────────────────────────────
    trainer: pl.Trainer = instantiate(
        cfg.trainer,
        callbacks=callbacks,
        logger=tb_logger,
        default_root_dir=output_dir,
    )

    # ─── 8) Launch training ───────────────────────────────────
    trainer.fit(task, datamodule=datamodule)


    # # 1) Load entire Hydra config
    # with initialize(config_path="conf", job_name="train"):
    #     cfg = compose(config_name="my_config")
    # logger.info(f"Configuration: {OmegaConf.to_yaml(cfg)}")

    # # 1.1) Patch optimizer_cls & scheduler_cls from str → actual class
    # cfg.task.optimizer_cls = _str_to_class(cfg.task.optimizer_cls)
    # cfg.task.scheduler_cls = _str_to_class(cfg.task.scheduler_cls)

    # # 2) Prepare paths & data
    # data_prefix = set_data_prefix()
    # output_dir = os.path.join(data_prefix, "output")
    # os.makedirs(output_dir, exist_ok=True)

    # splits_dir = os.path.join(data_prefix, "splits", trajectory_dir)
    # split_file = os.path.join(splits_dir, "inner_splits_0.npz")
    # if not os.path.exists(split_file):
    #     raise FileNotFoundError(
    #         f"Split file not found: {split_file}. Run your split script first."
    #     )
    # path_to_data = os.path.join(data_prefix, trajectory_dir, "md_trajectory.db")

    # datamodule = load_xtb_dataset(path_to_data, cfg, split_file)

    # # 3) Instantiate the model & task directly from the config
    # model = instantiate(cfg.model)

    # # The AtomisticTask in our cfg already knows optimizer, scheduler, loss, metrics
    # task = instantiate(cfg.task, model=model)

    # # 4) Instantiate callbacks and logger
    # callbacks = [
    #     instantiate(cfg.callbacks.model_checkpoint),
    #     instantiate(cfg.callbacks.early_stopping),
    #     instantiate(cfg.callbacks.lr_monitor),
    #     instantiate(cfg.callbacks.ema),
    # ]
    # tb_logger = instantiate(cfg.logger.tensorboard)

    # # 5) Instantiate the Trainer
    # trainer: pl.Trainer = instantiate(
    #     cfg.trainer,
    #     callbacks=callbacks,
    #     logger=tb_logger,
    #     default_root_dir=output_dir
    # )

    # # 6) Train
    # trainer.fit(task, datamodule=datamodule)

if __name__ == "__main__":
    args = parse_args()
    main(**args)
