import os
import argparse
import torch
import torchmetrics
import pytorch_lightning as pl
import schnetpack as spk
import spainn

from ase.db import connect

from ground_state_md.utils import set_data_prefix
from ground_state_md.setup_logger import setup_logger


logger = setup_logger(logging_level_str="debug")


# Example command to run the script from within code directory:
"""
python -m excited_state_md.training_and_inference.train_e_f
"""


def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Prepare TDDFT data for usage in SchNetPack")
    parser.add_argument("--target_dir", type=str, default="PREPARE_12", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12)")
    return vars(parser.parse_args())


def main(target_dir: str):
    """
    Main function to prepare XTB data in units of Angstrom and kcal/mol for usage in SchNetPack.
    Args:
        target_dir (str): Directory containing the GEO folders containing excited state trajectories generated by Turbomole.
        computed_cycles (int): Number of cycles for which the gradients were computed.
        num_atoms (int): Number of atoms in the simulation. Can be found in the "control" file under "natoms".
        position_unit (str): Target unit for positions to transform from atomic units to (default: angstrom).
        energy_unit (str): Target unit for energies to transform from atomic units to (default: kcal/mol).
        time_unit (str): Target unit for time to transform from atomic units to (default: fs).
    """
    # setup paths to the necessary files
    data_path = os.path.join(set_data_prefix(), target_dir)
    data_prefix = os.path.join(data_path, "GEO_100000", "test")
    #data_prefix = os.path.join(set_data_prefix(), "MOTOR_MD_XTB/T300_1")
    db_name = "md_trajectory_bohr_hartree_aut.db"
    data_path = os.path.join(data_prefix, db_name)

    db = connect(data_path)

    print(db.metadata)

    
    data_module = spainn.SPAINN(
        n_states = 2, # singlet states 0, 1
        n_nacs = 1, # couplings: 01
        datapath=data_path, # path to database
        batch_size=10,
        num_train=0.7, # 70% of databse for training
        num_val=0.1, # 10% of database for validation
        #split_file=os.path.join('train', 'split.npz'), # path to model (training folder)
        splitting=spk.data.splitting.RandomSplit(),
        load_properties=['energy', 'forces'],
        transforms=[
            # remove mean of energy in every electronic state
            spk.transform.RemoveOffsets("energy", remove_mean=True, remove_atomrefs=False), 
            spk.transform.ASENeighborList(cutoff=5.0),
            spk.transform.CastTo32(),
                ],
    )

    # setup everything
    data_module.prepare_data()
    data_module.setup()

    n_atom_basis = 30
    cutoff = 10.0

    # input module: calculates pairwise distances between atoms
    pairwise_distance = spk.atomistic.PairwiseDistances()

    # SchNet Representation
    radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)

    schnet = spk.representation.SchNet(
        n_atom_basis=n_atom_basis,
        n_interactions=3,
        radial_basis=radial_basis,
        cutoff_fn=spk.nn.CosineCutoff(cutoff)
    )

    pred_energy = spainn.Atomwise(
        n_in=n_atom_basis,
        n_out=2, # two singlet states
        output_key=spainn.SPAINN.energy,
        n_layers=3,
    )

    pred_forces = spainn.Forces(
        energy_key=spainn.SPAINN.energy,
        force_key=spainn.SPAINN.forces,
    )


    nnpot = spk.model.NeuralNetworkPotential(
        representation=schnet,
        input_modules=[pairwise_distance],
        output_modules=[pred_energy, pred_forces],
        input_dtype_str='float32',
        do_postprocessing=True,
        postprocessors=[
            spk.transform.CastTo64(),
            spk.transform.AddOffsets(spainn.SPAINN.energy, add_mean=True, add_atomrefs=False),
        ],
    )

    output_energy = spk.task.ModelOutput(
        name=spainn.SPAINN.energy,
        loss_fn=torch.nn.MSELoss(),
        loss_weight=0.01,
        metrics={
            "MAE": torchmetrics.MeanAbsoluteError(),
        },
    )

    output_forces = spk.task.ModelOutput(
        name=spainn.SPAINN.forces,
        loss_fn=torch.nn.MSELoss(),
        loss_weight=0.99,
        metrics={
            "MAE": torchmetrics.MeanAbsoluteError(),
        },
    )

    task = spk.task.AtomisticTask(
        model=nnpot,
        outputs=[output_energy, output_forces],
        optimizer_cls=torch.optim.AdamW,
        optimizer_args={"lr": 1e-4},
        scheduler_monitor="val_loss",
    )

    modelpath = os.path.join(os.getcwd(), 'train')
    callbacks = [
        spk.train.ModelCheckpoint(
            model_path=os.path.join(modelpath, "best_model_E_F"),
            save_top_k=1,
            monitor="val_loss"
        )
    ]

    import warnings; warnings.simplefilter('ignore')
    trainer = pl.Trainer(
        log_every_n_steps=1,
        callbacks=callbacks,
        logger=pl.loggers.TensorBoardLogger(save_dir=modelpath),
        default_root_dir=modelpath,
        max_epochs=1, # for testing, we restrict the number of epochs
    )

    trainer.fit(task, datamodule=data_module)

    
    

if __name__=="__main__":
    args = parse_args()
    main(**args)