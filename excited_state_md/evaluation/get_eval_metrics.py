import os
import argparse
import torch
import pytorch_lightning as pl
import numpy as np
import pandas as pd
import torch.multiprocessing as mp
mp.set_sharing_strategy('file_system') # to avoid problems with num_workers > 0

from omegaconf import OmegaConf, DictConfig
from schnetpack.utils.compatibility import load_model
from tqdm import tqdm

from ground_state_md.units import convert_energies, convert_forces, get_ase_units_from_str
from ground_state_md.utils import set_data_prefix, setup_datamodule, get_num_workers, set_data_units_in_config
from ground_state_md.setup_logger import setup_logger

# Similar functions as in ground_state_md.utils but with different (relative) paths
from excited_state_md.utils import load_config, get_split_path
from excited_state_md.utils import remove_splitting_lock_file 

BATCH_SIZE = 1

# Example command to run the script from within code directory:
"""
screen -dmS excited_state_eval sh -c 'python -m excited_state_md.evaluation.get_eval_metrics --trajectory_dir PREPARE_12/spainn_datasets --model_dir epochs_200_bs_16_lr_0.0001_flw_0.9_elw_0.05_nlw_0.05_seed_42 --units bohr_hartree_aut --evaluation_data val ; exec bash'
"""


logger = setup_logger("debug")

def parse_args() -> dict:
    """ 
    Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for predicting with trained model on TDDFT test data.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="PREPARE_12/spainn_datasets", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12/spainn_datasets)")
    parser.add_argument("--units", type=str, default="angstrom_kcal_per_mol_fs", choices=["angstrom_kcal_per_mol_fs", "angstrom_ev_fs", "angstrom_hartree_fs", "bohr_hartree_aut"], help="Units for the input data (default: angstrom_kcal_per_mol_fs).")
    parser.add_argument("--nr_of_dirs", type=int, default=53, help="Number of directories to process. (default: 53)")
    parser.add_argument("-mdir", "--model_dir", type=str, default="epochs_100_bs_32_lr_0.0001_flw_0.495_elw_0.01_nlw_0.495_seed_42", help="Directory of the trained model (default: epochs_100_bs_32_lr_0.0001_flw_0.495_elw_0.01_nlw_0.495_seed_42)")
    parser.add_argument("--evaluation_data", type=str, default="test", choices=["train", "val", "test"], help="Data to evaluate the model on (default: test)")
    parser.add_argument("-f", "--fold", type=int, default=0, help="Fold number for cross-validation (default: 0)")
    parser.add_argument("-s", "--seed", type=int, default=42, help="Random seed for reproducibility (default: 42)")
    parser.add_argument("-nw", "--num_workers", type=int, default=0, help="Number of workers for data loading (default: 0, which sets it to 0 on macOS and 8 on Linux)")
    return vars(parser.parse_args())


# practically the same as in ground_state_md.evaluation.get_eval_metrics.py
def update_config_with_train_config(cfg: DictConfig, cfg_train: DictConfig) -> DictConfig:
    """
    Update the configuration with the training configuration to ensure consistency.
    Args:
        cfg (DictConfig): The original configuration.
        cfg_train (DictConfig): The training configuration.
    Returns:
        DictConfig: Updated configuration.
    """
    # Disable struct mode to allow adding new fields
    OmegaConf.set_struct(cfg, False)

    # Copy over specific globals and of run
    cfg.globals.cutoff = cfg_train.globals.cutoff
    cfg.globals.energy_key = cfg_train.globals.energy_key
    cfg.globals.forces_key = cfg_train.globals.forces_key
    cfg.globals.nacs_key = cfg_train.globals.nacs_key
    cfg.globals.n_states = cfg_train.globals.n_states
    cfg.globals.n_nacs = cfg_train.globals.n_nacs
    cfg.run.mean_std_path = cfg_train.run.mean_std_path

    # Copy over data fields
    cfg.data = cfg_train.data
    # Create model field if it doesn't exist
    if 'model' not in cfg:
        cfg.model = DictConfig({})
    # Copy over model fields
    cfg.model.postprocessors = cfg_train.model.postprocessors

    # Re-enable struct mode to enforce the structure
    OmegaConf.set_struct(cfg, True)
    return cfg

def update_config(cfg: DictConfig, ase_unit_names: dict, run_path: str, num_workers: int) -> DictConfig:
    """ 
    Update the configuration with command-line arguments.
    Args:
        cfg (DictConfig): The original configuration.
        ase_unit_names (dict): A dictionary containing the ASE unit names for distance, energy, and forces.
        run_path (str): Path to save the run.
        num_workers (int): Number of workers for data loading.
    Returns:
        DictConfig: Updated configuration.
    """
    cfg.run.path = run_path
    cfg.data.num_workers = get_num_workers(num_workers)
    cfg.org_data.num_workers = get_num_workers(num_workers)  # Set number of workers for data loading
    cfg.org_data.batch_size = BATCH_SIZE  # Set batch size for inference

    # Set ASE units in the configuration
    cfg.org_data = set_data_units_in_config(cfg.org_data, ase_unit_names)

    return cfg

def collect_predictions_and_ground_truths(cfg: DictConfig, datamodule: pl.LightningDataModule, model: torch.nn.Module):
    """ Collect predictions and ground truths for energy and forces from the neural network model on the test data.
    Args:
        cfg (DictConfig): Configuration containing model and data parameters.
        datamodule: Data module containing the test data.
        model: The trained neural network model.
    Returns:
        dict: A dictionary containing predictions and ground truths for energy and forces.
    """
    results = {
        "predictions": {cfg.globals.energy_key: [], cfg.globals.forces_key: [], cfg.globals.nacs_key: []}, 
        "ground_truths": {cfg.globals.energy_key: [], cfg.globals.forces_key: [], cfg.globals.nacs_key: []}
    }
    count = 0
    for batch in tqdm(datamodule, desc="Evaluating model on test data"): 
        e_gt = batch[cfg.globals.energy_key].cpu().numpy()
        f_gt = batch[cfg.globals.forces_key].cpu().numpy()
        nac_gt = batch[cfg.globals.nacs_key].cpu().numpy() 
        # forward pass (gradients needed for forces)
        output = model(batch)
        # energy and forces required grads -> detach them
        e_pred = output[cfg.globals.energy_key].detach().cpu().numpy()
        f_pred = output[cfg.globals.forces_key].detach().cpu().numpy()
        nac_pred = output[cfg.globals.nacs_key].detach().cpu().numpy() 
        # store predictions and ground truths
        results["predictions"][cfg.globals.energy_key].append(e_pred)
        results["predictions"][cfg.globals.forces_key].append(f_pred)
        results["predictions"][cfg.globals.nacs_key].append(nac_pred)
        results["ground_truths"][cfg.globals.energy_key].append(e_gt)
        results["ground_truths"][cfg.globals.forces_key].append(f_gt)
        results["ground_truths"][cfg.globals.nacs_key].append(nac_gt)  

        # if count >= 10:
        #     break
        # count += 1

    return results

def transform_predictions_and_ground_truths_to_numpy(preds_and_gts: dict, cfg_globals: DictConfig, nr_atoms: int) -> dict:
    """ 
    Transform predictions and ground truths from lists to numpy arrays.
    Args:
        preds_and_gts (dict): A dictionary containing predictions and ground truths for energy and forces.
        cfg_globals (DictConfig): The globals configuration containing parameters used for training.
        nr_atoms (int): Number of atoms in each sample.
    Returns:
        dict: A dictionary containing predictions and ground truths as numpy arrays.
    """
    energy_key = cfg_globals.energy_key
    forces_key = cfg_globals.forces_key
    nacs_key = cfg_globals.nacs_key
    n_states = cfg_globals.n_states
    n_nacs = cfg_globals.n_nacs
    transformed = {
        "predictions": {
            energy_key: np.concatenate(preds_and_gts["predictions"][energy_key]),
            forces_key: np.concatenate(preds_and_gts["predictions"][forces_key]).reshape(-1, n_states, nr_atoms, 3),
            nacs_key: np.concatenate(preds_and_gts["predictions"][nacs_key]).reshape(-1, n_nacs, nr_atoms, 3)
        },
        "ground_truths": {
            energy_key: np.concatenate(preds_and_gts["ground_truths"][energy_key]),
            forces_key: np.concatenate(preds_and_gts["ground_truths"][forces_key]).reshape(-1, n_states, nr_atoms, 3),
            nacs_key: np.concatenate(preds_and_gts["ground_truths"][nacs_key]).reshape(-1, n_nacs, nr_atoms, 3)
        }
    }
    logger.debug(f"Transformed predictions shape: {transformed['predictions'][energy_key].shape}, {transformed['predictions'][forces_key].shape}, {transformed['predictions'][nacs_key].shape}")
    logger.debug(f"Transformed ground truths shape: {transformed['ground_truths'][energy_key].shape}, {transformed['ground_truths'][forces_key].shape}, {transformed['ground_truths'][nacs_key].shape}")
    return transformed

def check_shapes(preds_and_gts: dict, nr_samples: int, energy_key: str, forces_key: str, nacs_key: str):
    """ Check the shapes of predictions and ground truths.
    Args:
        preds_and_gts (dict): A dictionary containing predictions and ground truths for energy and forces.
        num_samples (int): Number of samples to check.
        energy_key (str): Key for energy in the predictions and ground truths.
        forces_key (str): Key for forces in the predictions and ground truths.
        nacs_key (str): Key for NACS in the predictions and ground truths.
    Raises:
        AssertionError: If the shapes of predictions and ground truths do not match the expected number of samples.
    """
    assert preds_and_gts["predictions"][energy_key].shape[0] == nr_samples, \
        f"Predicted energies shape mismatch: {preds_and_gts['predictions'][energy_key].shape[0]} != {nr_samples}"
    assert preds_and_gts["predictions"][forces_key].shape[0] == nr_samples, \
        f"Predicted forces shape mismatch: {preds_and_gts['predictions'][forces_key].shape[0]} != {nr_samples}"
    assert preds_and_gts["predictions"][nacs_key].shape[0] == nr_samples, \
        f"Predicted NACS shape mismatch: {preds_and_gts['predictions'][nacs_key].shape[0]} != {nr_samples}"
    assert preds_and_gts["ground_truths"][energy_key].shape[0] == nr_samples, \
        f"Ground truth energies shape mismatch: {preds_and_gts['ground_truths'][energy_key].shape[0]} != {nr_samples}"
    assert preds_and_gts["ground_truths"][forces_key].shape[0] == nr_samples, \
        f"Ground truth forces shape mismatch: {preds_and_gts['ground_truths'][forces_key].shape[0]} != {nr_samples}"
    assert preds_and_gts["ground_truths"][nacs_key].shape[0] == nr_samples, \
        f"Ground truth NACS shape mismatch: {preds_and_gts['ground_truths'][nacs_key].shape[0]} != {nr_samples}"
    logger.debug("Shapes of predictions and ground truths are correct.")

def compute_mae_with_std_error(groundtruths: np.ndarray, predictions: np.ndarray) -> tuple:
    """ Compute Mean Absolute Error (MAE) and its standard error for a given property.
    Args:
        groundtruths (np.ndarray): The ground truth values.
        predictions (np.ndarray): The predicted values.
    Returns:
        tuple: Mean Absolute Error (MAE) and its standard error for the specified property.
    """
    logger.debug(f"Shape of predictions: {predictions.shape}")
    abs_diff = np.abs(predictions - groundtruths)
    logger.debug(f"Shape of absolute differences: {abs_diff.shape}")
    mae = np.mean(abs_diff)
    mae_std_error = np.std(abs_diff) / np.sqrt(len(abs_diff))  # Standard error of the mean
    return mae, mae_std_error

def compute_mae_with_std_error_per_state(groundtruths: np.ndarray, predictions: np.ndarray) -> dict:
    """ Compute Mean Absolute Error (MAE) and its standard error for a given property and for each state separately.
    Args:
        groundtruths (np.ndarray): The ground truth values.
        predictions (np.ndarray): The predicted values.
    Returns:
        dict: A dictionary containing the MAE and its standard error for the specified property.
    """
    logger.debug(f"Shape of predictions: {predictions.shape}")
    nr_states = predictions.shape[1]  # assuming shape is (n_samples, n_states, ...)
    abs_diff = np.abs(predictions - groundtruths)
    logger.debug(f"Shape of absolute differences: {abs_diff.shape}")
    mae_per_state = {'mae': {}, 'std_error': {}}
    for state in range(nr_states):
        mae, mae_std_error = compute_mae_with_std_error(groundtruths[:, state], predictions[:, state])
        mae_per_state[f"S_{state}"] = {'mae': mae, 'std_error': mae_std_error}
    return mae_per_state

def convert_mae_dict_units(mae_dict_model_units: dict, convert_function: callable, model_unit: str, target_unit: str) -> dict:
    """ Convert the MAE and standard error from model units to target units.
    Args:
        mae_dict_model_units (dict): The MAE dictionary with standard errors in model units.
        convert_function (callable): The function to convert units.
        model_unit (str): The model units.
        target_unit (str): The target units.
    Returns:
        dict: The MAE dictionary with standard errors in specified target units.
    """
    nr_states = len(mae_dict_model_units['S_0'])
    mae_dict = {}
    for state in range(nr_states):
        mae = convert_function(mae_dict_model_units[f"S_{state}"]['mae'], model_unit, target_unit)
        std_error = convert_function(mae_dict_model_units[f"S_{state}"]['std_error'], model_unit, target_unit)
        mae_dict[f"S_{state}"] = {'mae': mae, 'std_error': std_error}
    return mae_dict

def main(trajectory_dir: str, units: str, nr_of_dirs: int, model_dir: str, evaluation_data: str, fold: int, seed: int, num_workers: int):
    pl.seed_everything(seed, workers=True)

    ####################### 1) Compose the config ###########################
    cfg = load_config("training_and_inference/conf", "inference_config", "inference")

    # set absolute and relative paths to the model directory
    project_root_dir = os.getcwd()
    runs_dir_path = os.path.join(project_root_dir, cfg.globals.runs_dir_subpath)
    model_dir_path = os.path.join(runs_dir_path, units, trajectory_dir.replace("/", "_"), model_dir)
    model_dir_rel_path = "".join(model_dir_path.split("excited_state_md/")[1:])
    logger.debug(f"Absolute path to the model directory: {model_dir_path}")
    logger.debug(f"Relative path to the model directory: {model_dir_rel_path}")

    # use training config to update the inference config
    train_cfg_path = os.path.join(model_dir_rel_path, cfg.globals.train_config_subpath)
    cfg_train = load_config(train_cfg_path, cfg.globals.hparams_file_name, "train")
    cfg = update_config_with_train_config(cfg, cfg_train)

    # update config with arguments from command line
    model_units = get_ase_units_from_str(units)
    logger.debug(f"Model units: {model_units}")
    cfg = update_config(cfg, model_units, model_dir_path, num_workers)
    logger.info(f"Loaded and updated config:\n{OmegaConf.to_yaml(cfg)}")

    energy_key = cfg.globals.energy_key
    forces_key = cfg.globals.forces_key
    nacs_key = cfg.globals.nacs_key
    logger.info(f"Energy key: {energy_key}, Forces key: {forces_key}, NACS key: {nacs_key}")
    ####################### 2) Prepare Data and Paths #########################
    data_prefix = set_data_prefix()
    split_file = get_split_path(data_prefix, trajectory_dir, fold)
    path_to_db = os.path.join(data_prefix, trajectory_dir, f"md_trajectory_{units}_{nr_of_dirs}_dirs_used.db")
    
    datamodule = setup_datamodule(data_cfg=cfg.org_data, datapath=path_to_db, split_file=split_file)
    if evaluation_data == "train":
        datamodule = datamodule.train_dataloader()
    elif evaluation_data == "val":
        datamodule = datamodule.val_dataloader()
    elif evaluation_data == "test":
        datamodule = datamodule.test_dataloader()

    len_test = len(datamodule.dataset)
    nr_atoms = datamodule.dataset[0]["_positions"].shape[0]
    logger.info(f"Number of {evaluation_data} samples: {len_test}")
    logger.info(f"Number of atoms in each sample: {nr_atoms}")
    
    ####################### 3) Collect predictions and ground truths of the test dataset ##############################    
    model = load_model(cfg.globals.model_path, device="cpu")
    model.eval()

    preds_and_gts = collect_predictions_and_ground_truths(cfg, datamodule, model)
    preds_and_gts = transform_predictions_and_ground_truths_to_numpy(preds_and_gts, cfg.globals, nr_atoms)
    # check_shapes(preds_and_gts, len_test, energy_key, forces_key, nacs_key) TODO

    ######################## 4) Compute metrics per state  ##############################
    logger.info("Computing metrics for the neural network model...")
    energy_gts, energy_preds = preds_and_gts['ground_truths'][energy_key], preds_and_gts['predictions'][energy_key]
    e_maes_model_units = compute_mae_with_std_error_per_state(energy_gts, energy_preds)
    forces_gts, forces_preds = preds_and_gts['ground_truths'][forces_key], preds_and_gts['predictions'][forces_key]
    f_maes_model_units = compute_mae_with_std_error_per_state(forces_gts, forces_preds)
    nacs_mae_model_units, nacs_mae_std_err_model_units = compute_mae_with_std_error(preds_and_gts['ground_truths'][nacs_key], preds_and_gts['predictions'][nacs_key])

    ######################### 5) Converts metrics to kcal/mol and angstrom ##############################
    ENERGY_UNIT = "kcal/mol"
    FORCE_UNIT = "kcal/mol/angstrom"
    model_energy_unit = model_units[energy_key].lower()
    model_forces_unit = model_units[forces_key].lower()
    model_distance_unit = model_units["distance"].lower()

    if 'smooth' in nacs_key:
        NACS_ENERGY_UNIT = "kcal/mol/angstrom"
        model_nacs_unit = f"{model_energy_unit}/{model_distance_unit}"
    else:
        NACS_ENERGY_UNIT = "1/angstrom"
        model_nacs_unit = f"1/{model_distance_unit}"

    logger.info(f"Model energy unit: {model_energy_unit}, forces unit: {model_forces_unit} and {nacs_key} unit: {model_nacs_unit}")
    logger.info(f"Target energy unit: {ENERGY_UNIT}, forces unit: {FORCE_UNIT} and {nacs_key} unit: {NACS_ENERGY_UNIT}")

    e_maes = convert_mae_dict_units(e_maes_model_units, convert_energies, model_energy_unit, ENERGY_UNIT)
    f_maes = convert_mae_dict_units(f_maes_model_units, convert_forces, model_forces_unit, FORCE_UNIT)
    nacs_mae = convert_forces(nacs_mae_model_units, model_nacs_unit, NACS_ENERGY_UNIT)
    nacs_mae_std_err = convert_forces(nacs_mae_std_err_model_units, model_nacs_unit, NACS_ENERGY_UNIT)

    ######################### 6) Log metrics and save them to a csv file ##############################
    # log energies in ev/angstrom and kcal/mol
    logger.info(f"Metrics on {evaluation_data} for nn:")
    for state in e_maes: # S_0, S_1, ...
        e_mae, e_mae_std_err = e_maes[state]['mae'], e_maes[state]['std_error']
        f_mae, f_mae_std_err = f_maes[state]['mae'], f_maes[state]['std_error']
        logger.info(f"{state} energy MAE: {e_mae:.6f} +/- {e_mae_std_err:.6f} {ENERGY_UNIT}")
        logger.info(f"{state} forces MAE: {f_mae:.6f} +/- {f_mae_std_err:.6f} {FORCE_UNIT}")
    logger.info(f"NACs MAE: {nacs_mae:.6f} +/- {nacs_mae_std_err:.6f} {NACS_ENERGY_UNIT}")
    # log energies in model units
    for state in e_maes:
        e_mae_model_units, e_mae_std_err_model_units = e_maes_model_units[state]['mae'], e_maes_model_units[state]['std_error']
        f_mae_model_units, f_mae_std_err_model_units = f_maes_model_units[state]['mae'], f_maes_model_units[state]['std_error']
        logger.info(f"{state} energy MAE in model units: {e_mae_model_units:.6f} +/- {e_mae_std_err_model_units:.6f} {model_energy_unit}")
        logger.info(f"{state} forces MAE in model units: {f_mae_model_units:.6f} +/- {f_mae_std_err_model_units:.6f} {model_forces_unit}")
    logger.info(f"NACs MAE in model units: {nacs_mae_model_units:.6f} +/- {nacs_mae_std_err_model_units:.6f} {model_nacs_unit}")

    # Save the metrics to a csv file
    results = {
        f"energy_mae_{ENERGY_UNIT}": e_mae,
        f"energy_mae_std_err_{ENERGY_UNIT}": e_mae_std_err,
        f"forces_mae_{FORCE_UNIT.replace("/", "_per_")}": f_mae,
        f"forces_mae_std_err_{FORCE_UNIT.replace("/", "_per_")}": f_mae_std_err,
        f"nacs_mae_{NACS_ENERGY_UNIT.replace("/", "_per_")}": nacs_mae,
        f"nacs_mae_std_err_{NACS_ENERGY_UNIT.replace("/", "_per_")}": nacs_mae_std_err
    }
    metrics_file = os.path.join(model_dir_path, f"{evaluation_data}_metrics_fold_{fold}.csv")
    df = pd.DataFrame([results])
    df.to_csv(metrics_file, index=False)
    logger.info(f"Metrics on {evaluation_data} saved to {metrics_file}")


if __name__ == "__main__":
    args = parse_args()
    main(**args)

    # remove splitting.lock file if it exists in cwd
    remove_splitting_lock_file()



# __main__ - INFO - Metrics on val for nn:
# __main__ - INFO - Energy MAE: 11.234570 +/- 0.088508 kcal/mol
# __main__ - INFO - Forces MAE: 23.149735 +/- 0.229969 kcal/mol/angstrom
# __main__ - INFO - NACs MAE: 2.333168 +/- 0.029598 kcal/mol/angstrom
# __main__ - INFO - Energy MAE in model units: 0.017903 +/- 0.000141 hartree
# __main__ - INFO - Forces MAE in model units: 0.019522 +/- 0.000194 hartree/bohr
# __main__ - INFO - NACs MAE in model units: 0.001968 +/- 0.000025 hartree/bohr
# __main__ - INFO - Metrics on val saved to /home/tof54964/whk/code/excited_state_md/training_and_inference/runs/bohr_hartree_aut/PREPARE_12_s
# painn_datasets/epochs_200_bs_16_lr_0.0001_flw_0.9_elw_0.05_nlw_0.05_seed_42/val_metrics_fold_0.csv