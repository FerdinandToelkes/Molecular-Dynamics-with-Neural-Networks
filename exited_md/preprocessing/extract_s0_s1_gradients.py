import os
import argparse

from tqdm import tqdm

from exited_md.preprocessing.utils import prepare_last_exited_cycles, set_path_and_remove_old_file
from md_with_schnet.utils import set_data_prefix
from md_with_schnet.setup_logger import setup_logger

# Example command to run the script from within code directory:
"""
python3 -m exited_md.preprocessing.extract_s0_s1_gradients --target_dir PREPARE_12
"""

# Note: sadly the data is messy :(. The ground state gradients were originally not computed when in the exited S1 state. Now, the gradients for the exited S1 state have been computed, but only for the first 1461 cycles.

# (*) The s0 and s1 gradients are scattered across the gradient, gradient_ex and s0_gradient files. The gradient file contains the gradients of the active state, the gradient_ex file contains the s1 gradients starting from the last exited cycle, and the s0_gradient file contains the gradients of the ground state at least up to the last exited cycle. The s1 gradients are extracted from the gradient file until the last exited cycle and from the gradient_ex file from the last exited cycle to the end of the trajectory. The s0 gradients are extracted from the s0_gradient file until the last exited cycle and the rest is taken from the gradient file. Unfortunately, this is a bit Lionel ...

logger = setup_logger(logging_level_str="info")

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Extract the gradients belonging to the ground state and exited energy for all GEO directories.")
    parser.add_argument("--target_dir", type=str, default="PREPARE_12", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12)")
    parser.add_argument("--computed_cycles", type=int, default=1461, help="Number of cycles for which the gradients were computed (default: 1461)")
    parser.add_argument("--total_cycles", type=int, default=3000, help="Total number of cycles in the trajectory (default: 3000)")
    parser.add_argument("--time_step", type=int, default=40, help="Time step in atomic units (au) used during data generation (can be found in mdlog files for example) (default: 40)")
    return vars(parser.parse_args())


def set_path_and_remove_old_gradients(data_path: str, geo_dir: str, name: str) -> str:
    """
    Set the output path for the gradients and remove any old files if they exist.
    Args:
        data_path (str): Path to the directory containing all the data, i.e. all the GEO directories.
        geo_dir (str): Name of directory for which the gradients are being processed.
        name (str): Name of the output file.
    Returns:
        str: The output path for the gradients file.
    """
    output_path = os.path.join(data_path, geo_dir, "test", name)
    if os.path.exists(output_path):
        logger.info(f"Removing old file {output_path}")
        os.remove(output_path)
    return output_path

def main(target_dir: str, computed_cycles: int, total_cycles: int, time_step: int):
    """
    Main function to extract s0 and s1 gradients from various files and save them to a text file.
    Args:
        target_dir (str): Directory where the log files are located.
    """
    # setup
    data_path = os.path.join(set_data_prefix(), target_dir)
    command_path = os.path.expanduser(f'~/whk/code/exited_md/preprocessing/extract_gradients.sh')
    logger.debug(f"data_path: {data_path}")
    
    # get all valid trajectories and the number of their last exited cycles
    last_exited_cycles = prepare_last_exited_cycles(data_path, computed_cycles)

    # exited cycle and then from ex_gradient file the rest
    for geo_dir, last_exited_cycle in tqdm(last_exited_cycles.items(), desc="Extracting gradients"):
        logger.info(f"Processing {geo_dir} with last exited cycle {last_exited_cycle}")
        s0_output_path = set_path_and_remove_old_file(data_path, geo_dir, "s0_gradients.txt")
        s1_output_path = set_path_and_remove_old_file(data_path, geo_dir, "s1_gradients.txt")

        # see (*) for explanation of the paths
        active_gradient_path = os.path.join(data_path, geo_dir, "test", "gradient")
        s0_gradient_path = os.path.join(data_path, "extra_ground_state_calculations", geo_dir, "ground_state_gradients.txt")
        s1_gradient_path = os.path.join(data_path, geo_dir, "test", "gradient_ex")
        if not os.path.exists(active_gradient_path) or not os.path.exists(s0_gradient_path) or not os.path.exists(s1_gradient_path):
            raise FileNotFoundError(f"File {active_gradient_path} or {s0_gradient_path} or {s1_gradient_path} does not exist. Skipping {geo_dir}.")

        # extract s0 gradients -> see (*) for explanation
        os.system(f"bash {command_path} {s0_gradient_path} 1 {last_exited_cycle} {time_step} >> {s0_output_path}")
        os.system(f"bash {command_path} {active_gradient_path} {last_exited_cycle+1} {total_cycles} {time_step} >> {s0_output_path}")

        # extract s1 gradients -> see (*) for explanation
        os.system(f"bash {command_path} {active_gradient_path} 1 {last_exited_cycle} {time_step} >> {s1_output_path}")
        os.system(f"bash {command_path} {s1_gradient_path} {last_exited_cycle+1} {total_cycles} {time_step} >> {s1_output_path}")
        logger.info(f"Extracted gradients for {geo_dir} with last exited cycle {last_exited_cycle} into {s0_output_path} and {s1_output_path}")
        


if __name__ == "__main__":
    args = parse_args()
    main(**args)
    
        