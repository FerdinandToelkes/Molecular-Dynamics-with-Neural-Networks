import os
import argparse
import numpy as np
import pandas as pd

from schnetpack.data import ASEAtomsData
from ase import Atoms
from ase.db import connect

from ground_state_md.units import convert_distances, convert_energies, convert_forces, convert_velocities, convert_nacs, convert_smooth_nacs, get_ase_unit_format
from ground_state_md.utils import set_data_prefix
from ground_state_md.setup_logger import setup_logger
from ground_state_md.preprocessing.prepare_xtb_data import get_trajectory_from_txt_and_reshape, get_atomic_numbers_from_xyz, get_overview_of_dataset

from exited_md.preprocessing.utils import prepare_last_exited_cycles


logger = setup_logger("info")


# Example command to run the script from within code directory:
"""
python -m exited_md.preprocessing.prepare_tddft_data  --num_atoms 48 --position_unit bohr --energy_unit hartree --time_unit aut
"""

# Note: 
# - SPaiNN needs the energies and forces to be in the following shapes:
#   - energies: (1, N_states) 
#   - forces: (N_atoms, N_states, R)
#   - nacs: (N_atoms, N_couplings, R)
#   with R=3, N_states=2 and N_couplings=1 for our case
# - The last mdlog file contains one extra cycle (marked by $current) 
#   -> warning will be logged when extracting the positions and velocities
#   -> just take the first n cycles, that are also in the energies.csv file (which does not contain this $current cycle)
# - Note that we only have the velocities for the active state

def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Prepare TDDFT data for usage in SchNetPack")
    parser.add_argument("--target_dir", type=str, default="PREPARE_12", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12)")
    parser.add_argument("--computed_cycles", type=int, default=1461, help="Number of cycles for which the gradients were computed (default: 1461)")
    parser.add_argument('--num_atoms', type=int, required=True, help='Number of atoms in the simulation. Can be found in the "control" file under "natoms".')
    parser.add_argument('--position_unit', type=str, default='angstrom', choices=['angstrom', 'bohr'], help='Target unit for positions to transform from atomic units to (default: angstrom)')
    parser.add_argument('--energy_unit', type=str, default='kcal/mol', choices=['kcal/mol', 'hartree', "ev"], help='Target unit for energies to transform from atomic units to (default: kcal/mol)')
    parser.add_argument('--time_unit', type=str, default='fs', choices=['fs', 'aut'], help='Target unit for time to transform from atomic units to (default: fs)')
    return vars(parser.parse_args())


def get_all_energies_from_csv(path: str) -> tuple:
    """
    Extract all (potential) energies for the S0 and S1 state from a .csv file and return them as a NumPy array.
    Args:
        path (str): Path to the .csv file.
    Returns:
        tuple: Tuple containing all energies and the number of samples.
    """
    df = pd.read_csv(path, comment='#')
    s0_energies = df['S0'].values
    s1_energies = df['S1'].values
    number_of_samples = df.shape[0]
    assert s0_energies.shape == s1_energies.shape, "S0 and S1 energies must have the same shape"
    logger.debug(f'number_of_samples: {number_of_samples}')
    return s0_energies, s1_energies, number_of_samples

def get_property_paths(data_prefix: str) -> dict:
    """
    Get paths to the necessary property files.
    Args:
        data_prefix (str): Path to the directory containing the trajectory data.
    Returns:
        dict: Dictionary containing paths to the necessary property files.
    """
    property_paths = {
        'traj': os.path.join(data_prefix, 'positions.txt'),
        'energy': os.path.join(data_prefix, 'energies.csv'),
        's0_grads': os.path.join(data_prefix, 's0_gradients.txt'),
        's1_grads': os.path.join(data_prefix, 's1_gradients.txt'),
        'nacs': os.path.join(data_prefix, 'nacs.txt'),
        'velocities': os.path.join(data_prefix, 'velocities.txt')
    }
    # check if all necessary files exist
    for key, path in property_paths.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"File {path} does not exist. Please check the path and ensure the file is present.\n"
            "Try running the extract scripts for the different properties.")
    return property_paths

def read_in_properties(property_paths, num_atoms) -> dict:
    """
    Read in all properties from the prepared .csv and .txt files.
    Args:
        property_paths (dict): Dictionary containing paths to the necessary property files.
        num_atoms (int): Number of atoms in the simulation.
    Returns:
        dict: Dictionary containing all properties.
    """
    logger.debug("Extracting data from prepared .csv and .txt files")
    s0_energy_traj, s1_energy_traj, number_of_samples  = get_all_energies_from_csv(property_paths['energy'])
    coords_traj = get_trajectory_from_txt_and_reshape(property_paths['traj'], number_of_samples, num_atoms, usecols=(1, 2, 3))
    s0_grads_traj = get_trajectory_from_txt_and_reshape(property_paths['s0_grads'], number_of_samples, num_atoms, usecols=(0, 1, 2))
    s1_grads_traj = get_trajectory_from_txt_and_reshape(property_paths['s1_grads'], number_of_samples, num_atoms, usecols=(0, 1, 2))
    nacs_traj = get_trajectory_from_txt_and_reshape(property_paths['nacs'], number_of_samples, num_atoms, usecols=(0, 1, 2))

    # only needed as starting point for later md simulations
    velocities_traj = get_trajectory_from_txt_and_reshape(property_paths['velocities'], number_of_samples, num_atoms, usecols=(1, 2, 3)) 
    atomic_numbers = get_atomic_numbers_from_xyz(property_paths['traj'], num_atoms, extra_lines=3)

    return {
        'coords': coords_traj,
        's0_energy': s0_energy_traj,
        's1_energy': s1_energy_traj,
        's0_grads': s0_grads_traj,
        's1_grads': s1_grads_traj,
        'nacs': nacs_traj,
        'velocities': velocities_traj,
        'atomic_numbers': atomic_numbers,
        'number_of_samples': number_of_samples
    }

def convert_to_target_units(properties: dict, position_unit: str, energy_unit: str, time_unit: str) -> dict:
    """
    Convert the properties to the target units.
    Args:
        properties (dict): Dictionary containing all properties.
        position_unit (str): Target unit for positions to transform from atomic units to (default: angstrom).
        energy_unit (str): Target unit for energies to transform from atomic units to (default: kcal/mol).
        time_unit (str): Target unit for time to transform from atomic units to (default: fs).
    Returns:
        dict: Dictionary containing all properties in the target units except the number of samples and the atomic
            numbers, since they need no converting and have already been extracted from the dict.
    """
    # covert from atomic units to desired units
    force_unit = f"{energy_unit}/{position_unit}"  # e.g., kcal/mol/Angstrom
    velocity_unit = f"{position_unit}/{time_unit}"  # e.g., Angstrom/fs
    nac_unit = f"1/{position_unit}"  # e.g., 1/Angstrom
    smooth_nac_unit = f"{energy_unit}/{position_unit}"  # e.g., kcal/mol/Angstrom
    logger.info(f"Converting units from atomic units to {position_unit}, {energy_unit} and {time_unit}")
    coords_traj = convert_distances(properties['coords'], from_units='bohr', to_units=position_unit)  
    s0_energy_traj = convert_energies(properties['s0_energy'], from_units='hartree', to_units=energy_unit)
    s1_energy_traj = convert_energies(properties['s1_energy'], from_units='hartree', to_units=energy_unit)
    s0_forces_traj = convert_forces(properties['s0_forces'], from_units='hartree/bohr', to_units=force_unit)
    s1_forces_traj = convert_forces(properties['s1_forces'], from_units='hartree/bohr', to_units=force_unit)
    nacs_traj = convert_nacs(properties['nacs'], from_units='1/bohr', to_units=nac_unit)
    smooth_nacs_traj = convert_smooth_nacs(properties['smooth_nacs'], from_units='hartree/bohr', to_units=smooth_nac_unit)
    velocities_traj = convert_velocities(properties['velocities'], from_units='bohr/aut', to_units=velocity_unit)
    return {
        'coords': coords_traj,
        's0_energy': s0_energy_traj,
        's1_energy': s1_energy_traj,
        's0_forces': s0_forces_traj,
        's1_forces': s1_forces_traj,
        'nacs': nacs_traj,
        'smooth_nacs': smooth_nacs_traj,
        'velocities': velocities_traj
    }

def reshape_to_spainn_format(properties: dict, number_of_samples: int, num_atoms: int) -> dict:
    """
    Reshape the properties to the format expected by SPaiNN 
    (see https://github.com/CompPhotoChem/SPaiNN/blob/main/tutorials/tut_01_preparing_data.ipynb).
    Args:
        properties (dict): Dictionary containing all properties.
        number_of_samples (int): Number of samples in the trajectory.
        num_atoms (int): Number of atoms in the simulation.
    Returns:
        dict: Dictionary containing all properties in the SPaiNN format.
    """
    combined_energy_traj = np.column_stack((properties['s0_energy'], properties['s1_energy']))
    combined_energy_traj = combined_energy_traj.reshape((number_of_samples, 1, 2)) # ensure (Nframes, N_states)
    combined_forces_traj = np.column_stack((properties['s0_forces'], properties['s1_forces']))  
    combined_forces_traj = combined_forces_traj.reshape((number_of_samples, num_atoms, 2, 3))  # ensure (Nframes, Natoms, N_states, 3)
    nacs_traj = properties['nacs'].reshape((number_of_samples, num_atoms, 1, 3))  # ensure (Nframes, Natoms, N_couplings, 3)
    smooth_nacs_traj = properties['smooth_nacs'].reshape((number_of_samples, num_atoms, 1, 3))  
    velocities_traj = properties['velocities'].reshape((number_of_samples, num_atoms, 1, 3))  # ensure (Nframes, Natoms, N_states, 3)
    # return an updated version of the properties dict 
    return {
        'coords': properties['coords'],
        'combined_energy': combined_energy_traj,
        'combined_forces': combined_forces_traj,
        'nacs': nacs_traj,
        'smooth_nacs': smooth_nacs_traj,
        'velocities': velocities_traj,
    }


def convert_trajectory_to_ase_and_append(property_list: list, atoms_list: list, properties: dict, atomic_numbers: list) -> tuple:
    """
    Convert current trajectory data to ASE Atoms objects and properties and append them to the lists.
    Args:
        property_list (list): List to store the properties.
        atoms_list (list): List to store the ASE Atoms objects.
        properties (dict): Dictionary containing the properties of the current trajectory.
        atomic_numbers (list): List of atomic numbers.
    Returns:
        tuple: Tuple containing a list of ASE Atoms objects and a list of properties.
    """
    logger.debug("Converting trajectory data to ASE Atoms objects and properties")

    for positions, combined_energies, combined_forces, nacs, smooth_nacs, velocities in zip(properties['coords'], properties['combined_energy'], properties['combined_forces'], properties['nacs'], properties['smooth_nacs'], properties['velocities']):
        ats = Atoms(positions=positions, numbers=atomic_numbers)
        # convert energies to array if it is not already
        if not isinstance(combined_energies, np.ndarray):
            combined_energies = np.array([combined_energies]) # compare with shape of data within the SchNetPack tutorial

        properties = {'energy': combined_energies, 'forces': combined_forces, 'nacs': nacs, 'smooth_nacs': smooth_nacs, 'velocities': velocities}
        property_list.append(properties)
        atoms_list.append(ats)

    return atoms_list, property_list

def convert_trajectory_to_ase(properties: dict, atomic_numbers: list) -> tuple:
    """
    Convert current trajectory data to ASE Atoms objects and properties and append them to the lists.
    Args:
        property_list (list): List to store the properties.
        atoms_list (list): List to store the ASE Atoms objects.
        properties (dict): Dictionary containing the properties of the current trajectory.
        atomic_numbers (list): List of atomic numbers.
    Returns:
        tuple: Tuple containing a list of ASE Atoms objects and a list of properties.
    """
    logger.debug("Converting trajectory data to ASE Atoms objects and properties")
    atoms_list = []
    property_list = []
    for positions, combined_energies, combined_forces, nacs, smooth_nacs, velocities in zip(properties['coords'], properties['combined_energy'], properties['combined_forces'], properties['nacs'], properties['smooth_nacs'], properties['velocities']):
        ats = Atoms(positions=positions, numbers=atomic_numbers)
        # convert energies to array if it is not already
        if not isinstance(combined_energies, np.ndarray):
            combined_energies = np.array([combined_energies]) # compare with shape of data within the SchNetPack tutorial

        properties = {'energy': combined_energies, 'forces': combined_forces, 'nacs': nacs, 'smooth_nacs': smooth_nacs, 'velocities': velocities}
        property_list.append(properties)
        atoms_list.append(ats)

    return atoms_list, property_list

def add_metadata_to_dataset(db_path: str, ase_units: dict, property_unit_dict: dict):
    """
    Add metadata required by SPaiNN to the dataset.
    Args:
        db_path (str): Path to the database file.
        ase_units (dict): Dictionary containing the units for distances, energies, time in ASE compatible format.
        property_unit_dict (dict): Dictionary containing the units for energies, forces, nacs, smooth_nacs, and velocities
        which has also been set in the ASEAtomsData.create() function.
    """
    metadata = {
            '_distance_unit': ase_units['distance'],
            '_property_unit_dict': property_unit_dict,
            'n_singlets': 2, 'n_doublets': 0, 'n_triplets': 0,
            'phasecorrected': False, 'states': 'S S', 'atomrefs': {}
        }
    db = connect(db_path)
    db.metadata = metadata

def write_used_dirs_to_info_file(used_dirs: list, info_path: str, num_samples: int, num_atoms: int, position_unit: str, energy_unit: str, time_unit: str, samples_per_geo_dir: int):
    """
    Write the used directories and metadata to the info file.
    Args:
        used_dirs (list): List of used directories.
        info_path (str): Path to the info file.
        num_samples (int): Number of samples in the trajectory.
        num_atoms (int): Number of atoms in the simulation.
        position_unit (str): Target unit for positions.
        energy_unit (str): Target unit for energies.
        time_unit (str): Target unit for time.
        samples_per_geo_dir (int): Number of samples per directory.
    """ 
    with open(info_path, 'w') as f:
        f.write(f"Number of samples in total: {num_samples}\n")
        f.write(f"Number of used directories: {len(used_dirs)}\n")
        f.write(f"Number of samples per directory: {samples_per_geo_dir}\n")
        f.write(f"Number of atoms: {num_atoms}\n")
        f.write(f"Position unit: {position_unit}\n")
        f.write(f"Energy unit: {energy_unit}\n")
        f.write(f"Time unit: {time_unit}\n")
        f.write(f"Used directories:\n")
        for geo_dir in used_dirs:
            f.write(f"{geo_dir}\n")


def main(target_dir: str, computed_cycles: int, num_atoms: int, position_unit: str, energy_unit: str, time_unit: str):
    """
    Main function to prepare XTB data in units of Angstrom and kcal/mol for usage in SchNetPack.
    Args:
        target_dir (str): Directory containing the GEO folders containing exited state trajectories generated by Turbomole.
        computed_cycles (int): Number of cycles for which the gradients were computed.
        num_atoms (int): Number of atoms in the simulation. Can be found in the "control" file under "natoms".
        position_unit (str): Target unit for positions to transform from atomic units to (default: angstrom).
        energy_unit (str): Target unit for energies to transform from atomic units to (default: kcal/mol).
        time_unit (str): Target unit for time to transform from atomic units to (default: fs).
    """
    # setup paths to the necessary files
    data_path = os.path.join(set_data_prefix(), target_dir)

    # get all valid trajectories and the number of their last exited cycles
    geo_dirs_with_last_exited_cycles = prepare_last_exited_cycles(data_path, computed_cycles)

    # setup paths for the new dataset    
    file_name = f"md_trajectory_{position_unit}_{energy_unit.replace('/', '_per_')}_{time_unit}"
    db_dir = os.path.join(data_path, "spainn_datasets")
    os.makedirs(db_dir, exist_ok=True)
    db_path = os.path.join(db_dir, f"{file_name}.db")
    info_path = os.path.join(db_dir, f"{file_name}_info.txt")

    if os.path.exists(db_path):
        logger.info(f"File {db_path} already exists, loading it.")
        new_dataset = ASEAtomsData(db_path)
        # get overview of the dataset
        get_overview_of_dataset(new_dataset)
        return 0


    # setup lists to store data in ase format
    atoms_list = []
    property_list = []
    props_all_dirs = None
    for geo_dir, _last_exited_cycle in geo_dirs_with_last_exited_cycles.items():
        logger.info(f"Processing GEO folder: {geo_dir}")
        # setup paths to the necessary files
        data_prefix = os.path.join(data_path, geo_dir, "test")
        property_paths = get_property_paths(data_prefix)

        # read in all properties from the prepared .csv and .txt files
        properties = read_in_properties(property_paths, num_atoms)
        number_of_samples = properties['number_of_samples']
        atomic_numbers = properties['atomic_numbers']

        # Convert gradients to forces and compute smooth NACs
        properties['s0_forces'] = - properties['s0_grads']
        properties['s1_forces'] = - properties['s1_grads']  
        # shape (N_samples, 1, 1), needed for numpy broadcasting
        energy_diff = (properties['s1_energy'] - properties['s0_energy'])[:, np.newaxis, np.newaxis] 
        properties['smooth_nacs'] = properties['nacs'] * energy_diff
        assert properties['smooth_nacs'].shape == properties['s0_forces'].shape, "smooth_nacs and s0_forces must have the same shape"
        
        # covert from atomic units to desired units
        properties = convert_to_target_units(properties, position_unit, energy_unit, time_unit)

        # bring data into the right shape for SPaiNN
        properties = reshape_to_spainn_format(properties, number_of_samples, num_atoms)

        if props_all_dirs is None:
            props_all_dirs = properties
        else:
            # combine properties from all directories
            for key in props_all_dirs.keys():
                props_all_dirs[key] = np.concatenate((props_all_dirs[key], properties[key]), axis=0)
            # check if each directory has the same number of samples
            assert samples_per_geo_dir == properties['coords'].shape[0], \
                f"Number of samples in {geo_dir} does not match the number of samples in the previous directories. " \
                f"Expected {samples_per_geo_dir}, got {properties['coords'].shape[0]}."

        samples_per_geo_dir = properties['coords'].shape[0]

        
    # convert trajectory data to ASE Atoms objects and properties
    logger.info(f"File {db_path} does not exist, creating it.")
    atoms_list, property_list = convert_trajectory_to_ase(props_all_dirs, atomic_numbers)

    # ase needs the distance unit in capitalized form
    ase_units = get_ase_unit_format(position_unit, energy_unit, time_unit)
    property_unit_dict = {
        'energy': ase_units['energy'], 'forces': ase_units['forces'], 'nacs': f"1/{ase_units['distance']}", 
        'smooth_nacs': f"{ase_units['energy']}/{ase_units['distance']}", 'velocities': ase_units['velocities']
    }
    # Create a new dataset in the SPaiNN format
    new_dataset = ASEAtomsData.create(db_path, distance_unit=ase_units['distance'], property_unit_dict=property_unit_dict)
    new_dataset.add_systems(property_list, atoms_list)

    # add metadata to the dataset
    add_metadata_to_dataset(db_path, ase_units, property_unit_dict)

    # write the used directories into the info file
    used_dirs = geo_dirs_with_last_exited_cycles.keys()
    write_used_dirs_to_info_file(used_dirs, info_path, len(atoms_list), num_atoms, position_unit, energy_unit, time_unit, samples_per_geo_dir)

    # get overview of the dataset
    get_overview_of_dataset(new_dataset)
    

if __name__=="__main__":
    args = parse_args()
    main(**args)