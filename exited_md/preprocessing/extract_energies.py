import os
import argparse
import subprocess
import csv
import re

from tqdm import tqdm

from exited_md.preprocessing.utils import prepare_last_exited_cycles, set_path_and_remove_old_file
from md_with_schnet.utils import set_data_prefix
from md_with_schnet.setup_logger import setup_logger

# Example command to run the script from within code directory:
"""
python3 -m exited_md.preprocessing.extract_energies --target_dir PREPARE_12
"""


logger = setup_logger(logging_level_str="info")

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Extract all the different energies into a text file for all GEO folders.")
    parser.add_argument("--target_dir", type=str, default="PREPARE_12", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12)")
    parser.add_argument("--computed_cycles", type=int, default=1461, help="Number of cycles for which the gradients were computed (default: 1461)")
    parser.add_argument("--total_cycles", type=int, default=3000, help="Total number of cycles in the trajectory (default: 3000)")
    parser.add_argument("--time_step", type=int, default=40, help="Time step in atomic units (au) used during data generation (can be found in mdlog files for example) (default: 40)")
    return vars(parser.parse_args())




def main(target_dir: str, computed_cycles: int, total_cycles: int, time_step: int):
    """
    Main function to extract all energies from different log files and save them to a text file.
    Args:
        target_dir (str): Directory where the log files are located.
    """
    # setup
    data_path = os.path.join(set_data_prefix(), target_dir)
    turbomole_command = f'log2egy'
    logger.debug(f"data_path: {data_path}")
    
    # get all valid trajectories and the number of their last exited cycles
    last_exited_cycles = prepare_last_exited_cycles(data_path, computed_cycles)

    # exited cycle and then from ex_gradient file the rest
    for geo_dir, last_exited_cycle in tqdm(last_exited_cycles.items(), desc="Extracting energies"):
        logger.info(f"Processing {geo_dir} with last exited cycle {last_exited_cycle}")
        output_path = set_path_and_remove_old_file(data_path, geo_dir, "energies.csv")
        path_to_mdlog_files = os.path.join(data_path, geo_dir, "test")
        if not os.path.exists(path_to_mdlog_files):
            logger.warning(f"Directory {path_to_mdlog_files} does not exist. Skipping {geo_dir}.")
            continue
        # run the command in the directory of mdlog files to extract energies
        result = subprocess.run([turbomole_command], cwd=path_to_mdlog_files, capture_output=True, text=True)
        lines = result.stdout.strip()

        if result.returncode != 0:
            logger.error(f"Error running {turbomole_command} in {path_to_mdlog_files}: {result.stderr}")
            continue

        if not lines:
            logger.warning(f"No lines returned from {turbomole_command} in {path_to_mdlog_files}. Skipping {geo_dir}. stderr: {result.stderr}")
            continue

        # extract the header: line starting with # t= (^ is beginning of line, \s matches any whitespace character)
        pattern = r"^\s*#\s*t="
        header_written = False

        # write the output to a csv file
        with open(output_path, 'w', newline='') as f:
            writer = csv.writer(f)

            for line in lines.splitlines():
                # extract the header only once
                # if not header_written and line.startswith(pattern):
                if not header_written and re.match(pattern, line):
                    # remove the # and split the line into parts
                    logger.info(f"Extracting header from line: {line}")
                    header = line.lstrip("#").split()
                    # write the header only once
                    writer.writerow(header)
                    header_written = True
                    continue
                # skip empty lines and comments
                if line.startswith("#") or not line.strip():
                    continue
                # split the line into parts and write to csv
                parts = line.split()
                writer.writerow(parts)

        
        break


if __name__ == "__main__":
    args = parse_args()
    main(**args)
    
        