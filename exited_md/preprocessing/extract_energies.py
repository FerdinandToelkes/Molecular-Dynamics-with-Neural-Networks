import os
import argparse
import subprocess
import csv
import re
import numpy as np
import pandas as pd

from tqdm import tqdm

from exited_md.preprocessing.utils import prepare_last_exited_cycles, set_path_and_remove_old_file
from ground_state_md.utils import set_data_prefix
from ground_state_md.setup_logger import setup_logger

# Example command to run the script from within code directory:
"""
python3 -m exited_md.preprocessing.extract_energies --target_dir PREPARE_12
"""

# (*) We work under the assumption, the only possible active states are S0 and S1, NOT S2.


logger = setup_logger(logging_level_str="info")

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Extract all the different energies into a text file for all GEO folders.")
    parser.add_argument("--target_dir", type=str, default="PREPARE_12", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12)")
    parser.add_argument("--computed_cycles", type=int, default=1461, help="Number of cycles for which the gradients were computed (default: 1461)")
    parser.add_argument("--total_cycles", type=int, default=3000, help="Total number of cycles that should be in each trajectory (default: 3000)")
    return vars(parser.parse_args())

##############################################################################################################
################# Functions to extract energies from mdlog files and write them to a CSV file ################
##############################################################################################################

def generate_energy_csv_from_mdlog_files(data_path: str, geo_dir: str, total_cycles: int, turbomole_command: str = 'log2egy') -> str:
    """
    Generate a CSV file containing energies from mdlog files in the specified directory.
    Args:
        data_path (str): Path to the directory containing all the data, i.e. all the GEO directories.
        geo_dir (str): Name of directory for which the energies are being processed.
        total_cycles (int): Total number of cycles that should be in each trajectory.
        turbomole_command (str): Command to run to extract energies from mdlog files (default: 'log2egy').
    Raises:
        FileNotFoundError: If the directory containing the mdlog files does not exist.
        RuntimeError: If the command fails to execute.
        ValueError: If the command returns no output, if the output is empty, or if the output does not have the expected number of lines.
    Returns:
        str: The path to the generated CSV file containing the energies.
    """
    output_path = set_path_and_remove_old_file(data_path, geo_dir, "energies.csv")

    # use turbomole command to extract energies from mdlog files        
    output_text = get_energies_from_mdlog_files(data_path, geo_dir, turbomole_command)

    # check if output has total_cycles lines, if not, raise an error
    output_lines = output_text.strip().splitlines()
    # remove comments
    output_lines = [line for line in output_lines if not line.startswith("#")]
    if len(output_lines) != total_cycles:
        raise ValueError(f"Expected {total_cycles} lines in the output, but got {len(output_lines)}. Please check the mdlog files in {os.path.join(data_path, geo_dir, 'test')} and ensure they contain the expected number of cycles.")
    # write the output to a csv file
    # ^ is beginning of line, \s matches any whitespace character
    write_energies_to_csv(output_text, output_path, header_pattern=r"^\s*#\s*t=")
    return output_path


def get_energies_from_mdlog_files(data_path: str, geo_dir: str, turbomole_command: str) -> str:
    """
    Run the command to extract energies from the log files in the specified directory and returns the output if successful.
    Args:
        data_path (str): Path to the directory containing all the data, i.e. all the GEO directories.
        geo_dir (str): Name of directory for which the energies are being processed.
        turbomole_command (str): Command to run to extract energies.
    Raises:
        FileNotFoundError: If the directory containing the mdlog files does not exist.
        RuntimeError: If the command fails to execute.
        ValueError: If the command returns no output or if the output is empty.
    Returns:
        str: The output text from the command.
    """
    path_to_mdlog_files = os.path.join(data_path, geo_dir, "test")
    if not os.path.exists(path_to_mdlog_files):
        raise FileNotFoundError(f"Directory {path_to_mdlog_files} does not exist. Skipping {geo_dir}.")

    # run the command in the directory of mdlog files to extract energies
    logger.info(f"Ensure to have TURBOMOLE installed and enabled in your environment. Running {turbomole_command} in {path_to_mdlog_files} to extract energies.")
    result = subprocess.run([turbomole_command], cwd=path_to_mdlog_files, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Error running {turbomole_command} in {path_to_mdlog_files}: {result.stderr}")
    
    # check if the output is empty
    output_text = result.stdout.strip()
    if not output_text:
        raise ValueError(f"No output_text returned from {turbomole_command} in {path_to_mdlog_files}. Skipping {geo_dir}. stderr: {result.stderr}")
    return output_text

def write_energies_to_csv(output_text: str, output_path: str, header_pattern: str = r"^\s*#\s*t="):
    """
    Write the extracted energies to a CSV file and extract the header from the output text.
    This function assumes that the output text contains lines starting with a header pattern.
    It writes the header only once and then writes the rest of the lines to the CSV file.
    Args:
        output_text (str): The output text containing the energies.
        output_path (str): The path to the output CSV file.
        header_pattern (str): Regular expression pattern to identify the header line.
    """
    # extract the header: line starting with # t= (^ is beginning of line, \s matches any whitespace character)
    header_written = False
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)

        for line in output_text.splitlines():
            # extract the header only once
            if not header_written and re.match(header_pattern, line):
                extract_and_set_header(writer, line)
                header_written = True
                continue

            # skip empty lines and comments
            if line.startswith("#") or not line.strip():
                continue
            # split the line into parts and write to csv
            parts = line.split()
            writer.writerow(parts)

def extract_and_set_header(writer: csv.writer, line: str):
    """
    Extract the header from the line and write it to the CSV writer.
    Args:
        writer (csv.writer): CSV writer object to write the header.
        line (str): Line from which to extract the header.
    """
    logger.debug(f"Extracting header from line: {line}")
    # remove the # and split the line into parts
    header = line.lstrip("#").split()
    # write the header only once
    writer.writerow(header)

##############################################################################################################
################# Functions to extract energies from ex_energies and add them to the CSV file ################
##############################################################################################################

def add_ex_energies_to_csv(data_path: str, geo_dir: str, output_path: str):
    """
    Load the ex_energies file and add the energies to the CSV file.
    Args:
        data_path (str): Path to the directory containing all the data, i.e. all the GEO directories.
        geo_dir (str): Name of directory for which the energies are being processed.
        output_path (str): Path to the output CSV file.
    Raises:
        FileNotFoundError: If the ex_energies file does not exist.
    """
    # load s0, s1-s0, s2-s0 from ex_energies and add them as new columns (s0, s1, s2)
    ex_energies_path = os.path.join(data_path, geo_dir, "test", "ex_energies")
    if not os.path.exists(ex_energies_path):
        raise FileNotFoundError(f"File {ex_energies_path} does not exist.")
    relative_ex_energies = read_ex_energies_file(ex_energies_path) # S0, S1-S0, S2-S0
    ex_energies = convert_ex_energies_to_absolute(relative_ex_energies) # S0, S1, S2

    # add the energies to the csv file as new columns
    df = pd.read_csv(output_path)
    if df.shape[0] != ex_energies.shape[0]:
        # remove the file and raise an error
        os.remove(output_path)
        raise ValueError(f"Number of rows in {output_path} ({df.shape[0]}) does not match number of rows in ex_energies ({ex_energies.shape[0]}). Removed the file {output_path}. Please check the ex_energies file {ex_energies_path} and ensure it has the same number of rows as the CSV file or add the folder {geo_dir} to the file specifying which directories to exclude from the preprocessing.")

    df[['S0', 'S1', 'S2']] = ex_energies
    df.to_csv(output_path, index=False)
    logger.debug(f"Added energies from {ex_energies_path} to {output_path}")

    # add comments to the first line of the CSV file
    add_comments_to_csv(output_path, data_path, geo_dir)

def read_ex_energies_file(path: str) -> np.ndarray:
    """
    Read the ex_energies file and extract the energies.
    Note: The energies are expected to be in the format:
    $ex_energies  ex energies
        cycle =      1    SCF energy =    -1113.9750159710 -> S0
        0.12735558357311D+00                               -> S1 - S0
        0.14620283576762D+00                               -> S2 - S0
        cycle =      2    SCF energy =    -1113.9690433550 -> S0
        0.12648625967876D+00                               -> S1 - S0
        0.14504225944702D+00                               -> S2 - S0
        ...
    Args:
        path (str): Path to the ex_energies file.
    Returns:
        np.ndarray: Array of energies extracted from the file.
    """
    energies = []
    with open(path, 'r') as f:
        for line in f:
            if 'SCF energy =' in line:
                # parse the SCF value
                scf_str = line.split('SCF energy =')[-1].strip()
                scf = float(scf_str)
                
                # read the next two lines, convert Fortran d or D -> E
                e1 = float(next(f).strip().replace('D', 'E').replace('d', 'E'))
                e2 = float(next(f).strip().replace('D', 'E').replace('d', 'E'))
                
                energies.append([scf, e1, e2])
    return np.array(energies)

def convert_ex_energies_to_absolute(rel_ex_energies: np.ndarray) -> np.ndarray:
    """
    Convert the relative energies from the ex_energies file to absolute energies
    and combine them into a single array.
    Args:
        rel_ex_energies (np.ndarray): Array of relative energies from the ex_energies file.
    Returns:
        np.ndarray: Array of absolute energies in the format [S0, S1, S2].
    """
    s1 = rel_ex_energies[:, 1] + rel_ex_energies[:, 0]  # S1 = S0 + (S1-S0)
    s2 = rel_ex_energies[:, 2] + rel_ex_energies[:, 0]  # S2 = S0 + (S2-S0)
    ex_energies = np.column_stack((rel_ex_energies[:, 0], s1, s2))  # S0, S1, S2
    return ex_energies

def add_comments_to_csv(output_path: str, data_path: str, geo_dir: str):
    """
    Add comments to the first line of the CSV file indicating where the energies were extracted from.
    Args:
        output_path (str): Path to the output CSV file.
        data_path (str): Path to the directory containing all the data, i.e. all the GEO directories.
        geo_dir (str): Name of directory for which the energies are being processed.
    """
    path_to_mdlog_files = os.path.join(data_path, geo_dir, "test")
    with open(output_path, 'r+') as f:
        content = f.read()
        # move the cursor to the beginning of the file
        f.seek(0)
        f.write(f"# Energies extracted from mdlog files with Turbomoles log2egy command executed in {path_to_mdlog_files}\n")
        f.write("# Added S0, S1, S2 energies from ex_energies file from the same folder.\n")
        f.write("# Note that the entries are in fortran format, i.e. D is used for exponentiation. This has to be changed later.\n")
        f.write("# All properties are in atomic units\n")
        # write the original content back
        f.write(content)

##############################################################################################################

def validate_energy_csv(output_path: str):
    """
    Validate the energy CSV file by checking for missing values and ensuring all columns have the same number of rows,
    as well as checking if the "PotentialE=" column is always equal to either S0 or S1.
    Args:
        output_path (str): Path to the output CSV file.
    Raises:
        ValueError: If any column has missing values or a different number of rows than expected.
        ValueError: If the "PotentialE=" column is not equal to S0 or S1.
    """
    # Read CSV while ignoring comment lines (lines starting with #)
    df = pd.read_csv(output_path, comment='#', skipinitialspace=True)
    nr_of_rows = df.shape[0]
    check_shape_and_missing_values(df, nr_of_rows, output_path)
    # check if the "PotentialE=" columns is always either equal to S0 or S1 (see (*))
    check_potential_energy_consistency(df, nr_of_rows, output_path)

def check_shape_and_missing_values(df: pd.DataFrame, nr_of_rows: int, output_path: str):
    """
    Check if the DataFrame has missing values and if all columns have the same number of rows.
    Args:
        df (pd.DataFrame): DataFrame to check.
        nr_of_rows (int): Expected number of rows in the DataFrame.
        output_path (str): Path to the output CSV file.
    Raises:
        ValueError: If any column has missing values or a different number of rows than expected.
    """
    for col in df.columns:
        if df[col].isnull().any():
            raise ValueError(f"Column {col} in {output_path} has missing values.")
        if df[col].shape[0] != nr_of_rows:
            raise ValueError(f"Column {col} in {output_path} has a different number of rows ({df[col].shape[0]}) than the other columns ({nr_of_rows}).")

def check_potential_energy_consistency(df: pd.DataFrame, nr_of_rows: int, output_path: str):
    """
    Check if the "PotentialE=" column is always equal to either S0 or S1 (see (*)).
    Args:
        df (pd.DataFrame): DataFrame containing the energies.
        nr_of_rows (int): Expected number of rows in the DataFrame.
        output_path (str): Path to the output CSV file.
    Raises:
        ValueError: If any row in the "PotentialE=" column is not equal to S0 or S1.
    """
    # check if the "PotentialE=" columns is always either equal to S0 or S1 (see (*))
    diff_s0 = df['PotentialE='] - df['S0']
    diff_s1 = df['PotentialE='] - df['S1']
    cols_equal_s0 = df[np.abs(diff_s0) < 1e-6].index
    cols_equal_s1 = df[np.abs(diff_s1) < 1e-6].index
    if nr_of_rows != len(cols_equal_s0) + len(cols_equal_s1):
        raise ValueError(f"Some rows in {output_path} have PotentialE= not equal to S0 or S1. Rows not equal to S0 or S1:\n{set(df.index) - set(cols_equal_s0) - set(cols_equal_s1)}")        

##############################################################################################################

def main(target_dir: str, computed_cycles: int, total_cycles: int):
    """
    Main function to extract all energies from different log files and save them to a text file.
    Args:
        target_dir (str): Directory where the log files are located.
    """
    # setup
    data_path = os.path.join(set_data_prefix(), target_dir)
    logger.debug(f"data_path: {data_path}")
    
    # get all valid trajectories and the number of their last exited cycles
    last_exited_cycles = prepare_last_exited_cycles(data_path, computed_cycles)

    # exited cycle and then from ex_gradient file the rest
    for geo_dir, _last_exited_cycle in tqdm(last_exited_cycles.items(), desc="Extracting energies"):
        # use turbomole command to extract energies from mdlog files
        output_path = generate_energy_csv_from_mdlog_files(data_path, geo_dir, total_cycles, turbomole_command='log2egy')

        # load s0, s1-s0, s2-s0 from ex_energies and add them as new columns (s0, s1, s2)
        add_ex_energies_to_csv(data_path, geo_dir, output_path)

        # check if everything is ok with the csv file
        validate_energy_csv(output_path)
        logger.debug(f"Successfully processed {geo_dir}. Energies saved to {output_path}")  


        


if __name__ == "__main__":
    args = parse_args()
    main(**args)
    
        