import os
import argparse

from md_with_schnet.utils import set_data_prefix
from md_with_schnet.setup_logger import setup_logger

# Example command to run the script from within code directory:
"""
python3 -m exited_md.preprocessing.extract_s1_gradients --target_dir PREPARE_12
"""

# Note: sadly the data is messy :(. The ground state gradients were originally not computed when in the exited S1 state. Now, the gradients for the exited S1 state have been computed, but only for the first 1461 cycles.

logger = setup_logger(logging_level_str="debug")

def parse_args() -> dict:
    """ Parse command-line arguments. 

    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Extract the gradients belonging to the exited S1 energy.")
    parser.add_argument("--target_dir", type=str, default="PREPARE_12", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12)")
    parser.add_argument("--computed_cycles", type=int, default=1461, help="Number of cycles for which the gradients were computed (default: 1461)")
    return vars(parser.parse_args())

def read_last_exited_cycles(data_path: str) -> dict:
    """
    Read the last exited cycles from the file and return them as a dictionary.
    Args:
        data_path (str): Path to the directory containing the last exited cycles file.
    Returns:
        dict: Dictionary with keys as geometry directories and values as the last exited cycle number.
    """
    last_exited_cycles = {}
    with open(os.path.join(data_path, "last_exited_cycle_of_valid_trajectories.txt"), 'r') as f:
        for line in f:
            geo_dir, last_cycle = line.strip().split(": ")
            last_exited_cycles[geo_dir] = int(last_cycle)
    logger.info(f"Number of generally valid trajectories: {len(last_exited_cycles)}")
    return last_exited_cycles

def delete_invalid_cycles(last_exited_cycles: dict, computed_cycles: int) -> dict:
    """
    Delete entries in last_exited_cycles that have a last exited cycle larger than the computed cycles.
    Args:
        last_exited_cycles (dict): Dictionary with keys as geometry directories and values as the last exited cycle number.
        computed_cycles (int): The number of cycles for which the gradients were computed.
    """
    for geo_dir in list(last_exited_cycles.keys()):
        if last_exited_cycles[geo_dir] > computed_cycles:
            logger.debug(f"Deleting {geo_dir} with last exited cycle {last_exited_cycles[geo_dir]} larger than {computed_cycles}")
            del last_exited_cycles[geo_dir]
    logger.info(f"Number of trajectories after deleting cycles larger than {computed_cycles}: {len(last_exited_cycles)}")
    return last_exited_cycles

def read_missing_files(data_path: str) -> list:
    """
    Read the missing files from the file and return them as a list.
    Args:
        data_path (str): Path to the directory containing the missing files file.
    Returns:
        list: List of geometry directories that are missing.
    """
    missing_files = []
    with open(os.path.join(data_path, "ALL_GROUND_STATE_GRADIENTS", "missing_files"), 'r') as f:
        for line in f:
            if line.strip() != "":
                # add the line to missing_files if it is not empty
               missing_files.append(line.strip())
    logger.debug(f"missing_files: {missing_files}")
    return missing_files

def delete_missing_files(last_exited_cycles: dict, missing_files: list) -> dict:
    """
    Delete entries in last_exited_cycles that are in the missing_files list.
    Args:
        last_exited_cycles (dict): Dictionary with keys as geometry directories and values as the last exited cycle number.
        missing_files (list): List of geometry directories that are missing.
    Returns:
        dict: Updated dictionary with entries removed that are in the missing_files list.
    """
    for geo_dir in missing_files:
        if geo_dir in last_exited_cycles:
            logger.debug(f"Deleting {geo_dir} from last_exited_cycles because it is in missing_files")
            del last_exited_cycles[geo_dir]
    logger.info(f"Number of trajectories after deleting missing files and invalid cycles: {len(last_exited_cycles)}")
    return last_exited_cycles

def main(target_dir: str, computed_cycles: int):
    """
    Main function to extract positions, gradients or velocities from log files and save them to a text file.
    Args:
        target_dir (str): Directory where the log files are located.
    """
    # setup
    data_path = os.path.join(set_data_prefix(), target_dir)
    logger.debug(f"data_path: {data_path}")
    
    # read in the last_exited_cycle_of_valid_trajectories.txt and extract the last exited cycles
    last_exited_cycles = read_last_exited_cycles(data_path)

    # delete all entries corresponding to cycles larger than the computed cycles
    last_exited_cycles = delete_invalid_cycles(last_exited_cycles, computed_cycles)

    # read in the "missing_files" file (format is GEO_1\n, GEO_2\n, ...)
    missing_files = read_missing_files(data_path)

    # delete all entries in last_exited_cycles that are in missing_files
    last_exited_cycles = delete_missing_files(last_exited_cycles, missing_files)

    # at future ferdi: now extract the s1 gradients from gradient file until the last 
    # exited cycle and then from ex_gradient file the rest
    for geo_dir, last_cycle in last_exited_cycles.items():
        logger.info(f"Processing {geo_dir} with last exited cycle {last_cycle}")
        # read the gradient file
        path_to_gradients = os.path.join(data_path, geo_dir, "test", "gradient")
        if not os.path.exists(path_to_gradients):
            logger.warning(f"File {path_to_gradients} does not exist. Skipping {geo_dir}.")
            continue
        
        gradients = np.loadtxt(path_to_gradients, usecols=(0, 1, 2, 3, 4))  # Assuming columns are t, g_x, g_y, g_z, s1_g_x
        logger.info(f"Loaded gradients from {path_to_gradients}, shape: {gradients.shape}")

        # filter gradients until the last exited cycle
        filtered_gradients = gradients[:last_cycle + 1]  # +1 to include the last cycle
        logger.debug(f"Filtered gradients shape: {filtered_gradients.shape}")

        # save the filtered gradients to a new file
        output_path = os.path.join(data_path, geo_dir, "test", "s1_gradients.txt")
        np.savetxt(output_path, filtered_gradients)
        logger.info(f"Saved filtered S1 gradients to {output_path}")

if __name__ == "__main__":
    args = parse_args()
    main(**args)
    
        