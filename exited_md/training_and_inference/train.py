import os
import argparse
import torch
import pytorch_lightning as pl


from ase.db import connect
from hydra.utils import instantiate, get_class
from omegaconf import OmegaConf, DictConfig

from exited_md.utils import get_split_path, remove_splitting_lock_file, load_config
from ground_state_md.utils import set_data_prefix, get_num_workers, setup_datamodule
from ground_state_md.setup_logger import setup_logger
from ground_state_md.units import get_ase_units_from_str, convert_distances
from ground_state_md.training_and_inference.train import get_data_paths


logger = setup_logger(logging_level_str="debug")

# see below script for timing details
torch.set_float32_matmul_precision('highest')


# Example command to run the script from within code directory:
"""
screen -dmS tddft_train sh -c 'python -m exited_md.training_and_inference.train --trajectory_dir PREPARE_12/spainn_datasets --units angstrom_kcal_per_mol_fs -e 100 ; exec bash'
"""

# or smaller for debugging:
"""
python -m exited_md.training_and_inference.train --trajectory_dir PREPARE_12/spainn_datasets/toy_data --units bohr_hartree_aut -e 1
"""


def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for training SPaiNN on TDDFT datasets.")
    # paths setup
    parser.add_argument("--trajectory_dir", type=str, default="PREPARE_12/spainn_datasets", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12/spainn_datasets)")
    parser.add_argument("--units", type=str, default="angstrom_kcal_per_mol_fs", choices=["angstrom_kcal_per_mol_fs", "angstrom_ev_fs", "angstrom_hartree_fs", "bohr_hartree_aut"], help="Units for the input data (default: angstrom_kcal_per_mol_fs).")
    # training setup
    parser.add_argument("-bs", "--batch_size", type=int, default=32, help="Batch size for training (default: 32)")
    parser.add_argument("-e", "--num_epochs", type=int, default=1, help="Number of epochs for training (default: 1)")
    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-4, help="Learning rate for the optimizer (default: 1e-4)")
    parser.add_argument("-flw", "--forces_loss_weight", type=float, default=0.495, help="Weight for the forces loss (default: 0.495). Default value works well for angstrom_kcal_per_mol_fs units, but may need to be adjusted for other units. Note that, they do NOT need to sum to 1.0 (see notes on change of units).")
    parser.add_argument("-elw", "--energy_loss_weight", type=float, default=0.01, help="Weight for the energy loss (default: 0.01). Default value works well for angstrom_kcal_per_mol_fs units, but may need to be adjusted for other units. Note that, they do NOT need to sum to 1.0 (see notes on change of units).")
    parser.add_argument("-nlw", "--nacs_loss_weight", type=float, default=0.495, help="Weight for the NACS loss (default: 0.495). Default value works well for angstrom_kcal_per_mol_fs units, but may need to be adjusted for other units. Note that, they do NOT need to sum to 1.0 (see notes on change of units).")
    parser.add_argument("-nw", "--num_workers", type=int, default=-1, help="Number of workers for data loading (default: -1, which sets it to 0 on macOS and 8 on Linux)")
    # others
    parser.add_argument("-cname", "--config_name", type=str, default="default_train_config", help="Name of the configuration file (default: default_train_config)")
    parser.add_argument("-f", "--fold", type=int, default=0, help="Fold number for cross-validation (default: 0)")
    parser.add_argument("-s", "--seed", type=int, default=42, help="Random seed for reproducibility (default: 42)")
    return vars(parser.parse_args())


####################################################################################
### Functions mostly copied from md_with_schnet.training_and_inference.train.py ####
####################################################################################
def set_run_path(trajectory_dir: str, units: str, num_epochs: int, batch_size: int, learning_rate: float, forces_loss_weight: float, energy_loss_weight: float, nacs_loss_weight: float, seed: int) -> str:
    """ 
    Set the run path based on the trajectory directory.
    Args:
        trajectory_dir (str): Directory containing the trajectory data.
        units (str): Units for the input data.
        num_epochs (int): Number of epochs for training.
        batch_size (int): Batch size for training.
        learning_rate (float): Learning rate for the optimizer.
        forces_loss_weight (float): Weight for the forces loss.
        energy_loss_weight (float): Weight for the energy loss.
        nacs_loss_weight (float): Weight for the NACS loss.
        seed (int): Random seed for reproducibility.
    Returns:
        str: The run path.
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    run_name = f"epochs_{num_epochs}_bs_{batch_size}_lr_{learning_rate}_flw_{forces_loss_weight}_elw_{energy_loss_weight}_nlw_{nacs_loss_weight}_seed_{seed}"
    traj_dir = trajectory_dir.replace("/", "_")
    run_path = os.path.join(current_dir, "runs", units, traj_dir, run_name)
    return run_path

def update_config(cfg: DictConfig, run_path: str, ase_units: dict, batch_size: int, num_epochs: int, learning_rate: float, forces_loss_weight: float, energy_loss_weight: float, nacs_loss_weight: float, num_workers: int, path_to_stats: str) -> DictConfig:
    """ 
    Update the configuration with command-line arguments.
    Args:
        cfg (DictConfig): The original configuration.
        run_path (str): Path to save the run.
        ase_units (dict): Dictionary containing ASE units.
        batch_size (int): Batch size for training.
        num_epochs (int): Number of epochs for training.
        learning_rate (float): Learning rate for the optimizer.
        forces_loss_weight (float): Weight for the forces loss.
        energy_loss_weight (float): Weight for the energy loss.
        nacs_loss_weight (float): Weight for the NACS loss.
        num_workers (int): Number of workers for data loading.
        path_to_stats (str): Path to the statistics file (mean and std) for standardization.
    Returns:
        DictConfig: Updated configuration.
    """
    cfg.run.path = run_path
    cfg.run.mean_std_path = path_to_stats
    cfg.data.batch_size = batch_size
    cfg.data.num_workers = get_num_workers(num_workers)
    cfg.trainer.max_epochs = num_epochs
    cfg.globals.lr = learning_rate

    if cfg.task.outputs[0].name != cfg.globals.energy_key or cfg.task.outputs[1].name != cfg.globals.forces_key or cfg.task.outputs[2].name != cfg.globals.nacs_key:
        raise ValueError(f"The task outputs names must be '{cfg.globals.energy_key}', '{cfg.globals.forces_key}', and '{cfg.globals.nacs_key}'. Please check your configuration file.")
    else:
        cfg.task.outputs[0].loss_weight = energy_loss_weight
        cfg.task.outputs[1].loss_weight = forces_loss_weight
        cfg.task.outputs[2].loss_weight = nacs_loss_weight

    # Set the ASE units in the configuration
    cfg.data.distance_unit = ase_units['distance']
    cfg.data.property_units.energy = ase_units['energy']
    cfg.data.property_units.forces = ase_units['forces']

    # Convert the cutoff distance to the specified distance unit
    cfg.globals.cutoff = convert_distances(cfg.globals.cutoff, "bohr", ase_units['distance'].lower())
    return cfg

####################################################################################


def main(trajectory_dir: str, units: str, batch_size: int, num_epochs: int, learning_rate: float, 
         forces_loss_weight: float, energy_loss_weight: float, nacs_loss_weight: float, num_workers: int, config_name: str, fold: int, seed: int):
    ####################### Basic setup ###########################
    pl.seed_everything(seed, workers=True)
    data_prefix = set_data_prefix()
    path_to_traj_dir = os.path.join(data_prefix, trajectory_dir)
    split_file = get_split_path(path_to_traj_dir, fold=0)
    path_to_db, path_to_stats = get_data_paths(data_prefix, trajectory_dir, fold, units)
    logger.info(f"Using the following units: {units}")

    ####################### 1) Compose the config ###########################
    cfg = load_config(f"training_and_inference/conf", config_name, "train")

    # set run path
    run_path = set_run_path(trajectory_dir, units, num_epochs, batch_size, learning_rate, forces_loss_weight, energy_loss_weight, nacs_loss_weight, seed)
    logger.debug(f"Run path: {run_path}")
    
    # update the config with the arguments from the command line
    ase_units = get_ase_units_from_str(units)
    cfg = update_config(cfg, run_path, ase_units, batch_size, num_epochs, learning_rate, 
                        forces_loss_weight, energy_loss_weight, nacs_loss_weight, num_workers, path_to_stats)
    logger.info(f"Loaded and updated config:\n{OmegaConf.to_yaml(cfg)}")

    ####################### 2) Switch strings to classes ####################
    optim_cls = get_class(cfg.task.optimizer_cls)  
    sched_cls = get_class(cfg.task.scheduler_cls)

    ####################### 3) Prepare our own data #########################
    # TODO: write funtion to check if metadata matches the config
    db = connect(path_to_db)
    logger.info(f"Connected to database: {path_to_db}")
    logger.info(f"Database metadata: {db.metadata}")
    
    datamodule = setup_datamodule(cfg.data, path_to_db, split_file)
    # this prints one line, like this:
    # {'energy': (tensor([-23.2072, -23.2048], dtype=torch.float64), tensor([0.0004, 0.0005], dtype=torch.float64))}
    # but it also happens in the spainn tutorial :)

    ####################### 4) Instantiate model & task from YAML ###########
    model: pl.LightningModule = instantiate(cfg.model)
    task: pl.LightningModule = instantiate(
        cfg.task,
        model=model,
        optimizer_cls=optim_cls,
        scheduler_cls=sched_cls
    )

    ####################### 5) Logger ################################
    # Convert the config to a basic dict with resolved values, i.e. ${work_dir} -> /path/to/work_dir
    used_config = OmegaConf.to_container(cfg, resolve=True)

    # instantiate the logger
    tb_logger = instantiate(cfg.logger.tensorboard)

    # Manually log your clean hyperparameters
    tb_logger.log_hyperparams(used_config) # saves as hparams.yaml (difficult to change save name)

    # Prevent Lightning from auto‐saving used_config (no‐op override)
    # use lambda function taking any number of arguments and returning None
    tb_logger.log_hyperparams = lambda *args, **kwargs: None

    ####################### 6) Callbacks ##############################
    callbacks = [
        instantiate(cfg.callbacks.model_checkpoint),
        instantiate(cfg.callbacks.early_stopping),
        instantiate(cfg.callbacks.lr_monitor),
        instantiate(cfg.callbacks.ema),
    ]

    ####################### 7) Trainer ################################
    trainer: pl.Trainer = instantiate(
        cfg.trainer,
        callbacks=callbacks,
        logger=tb_logger,
    )

    ####################### 8) Launch training ########################
    # since we have a pl datamodule, it is split automatically into train, val and test sets
    # depending on which method is called, e.g. fit, validate, test
    # see https://lightning.ai/docs/pytorch/stable/data/datamodule.html for more details
    trainer.fit(task, datamodule=datamodule)
    
        
    

if __name__=="__main__":
    args = parse_args()
    import time
    start = time.time()
    main(**args)
    end = time.time()
    logger.debug(f"Total time taken for main(): {end - start:.4f} seconds")

    # remove splitting.lock file if it exists in cwd
    remove_splitting_lock_file()


# using torch.set_float32_matmul_precision(precision) with 2 epochs, 2300 + 300 data points and with batch size 100
# precision='highest': 
# t_total = 18.7097 s
# precision='high': 
# t_total = 18.7844 s
# precision='medium': 
# t_total = 18.6141 s
# => use highest precision for training and batch size 100

# testing different batch sizes with 2 epochs, 2300 + 300 data points and with precision='highest'
# batch_size = 10:
# t_total = 36.0209 s
# About 55% GPU usage
# batch_size = 25:
# t_total = 17.8927 s
# About 95% GPU usage
# batch_size = 32:
# t_total = 17.1538 s
# About 98% GPU usage and memory usage is around 4.3/15.9 GiB
# batch_size = 50:
# t_total = 17.6598 s
# Full GPU usage but seem less erratic than with batch_size=100
# batch_size = 100:
# t_total = 18.7097 s
# Full GPU usage

# => use batch size 32, because GPU usage is high but not at 100% and its also a power of 2

# timing on full dataset with one epoch and batch size 32 and highest precision
# training with 75000, validation with 9000 data points
# t_total = 153.6782 s