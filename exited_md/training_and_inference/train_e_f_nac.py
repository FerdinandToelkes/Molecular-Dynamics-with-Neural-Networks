import os
import argparse

from ase.db import connect

import torch
torch.set_float32_matmul_precision('highest') # see below script for timing details
import torchmetrics
import pytorch_lightning as pl
import schnetpack as spk

import spainn

from exited_md.utils import get_split_path, remove_splitting_lock_file
from md_with_schnet.utils import set_data_prefix
from md_with_schnet.setup_logger import setup_logger


logger = setup_logger(logging_level_str="debug")


# Example command to run the script from within code directory:
"""
python -m exited_md.training_and_inference.train_e_f_nac --trajectory_dir PREPARE_12/spainn_datasets/toy_data
"""


def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Prepare TDDFT data for usage in SchNetPack")
    parser.add_argument("--trajectory_dir", type=str, default="PREPARE_12/spainn_datasets", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12/spainn_datasets)")
    return vars(parser.parse_args())


def main(trajectory_dir: str):
    """
    Main function to prepare XTB data in units of Angstrom and kcal/mol for usage in SchNetPack.
    Args:
        trajectory_dir (str): Directory containing the .db file with the trajectory data.
        computed_cycles (int): Number of cycles for which the gradients were computed.
        num_atoms (int): Number of atoms in the simulation. Can be found in the "control" file under "natoms".
        position_unit (str): Target unit for positions to transform from atomic units to (default: angstrom).
        energy_unit (str): Target unit for energies to transform from atomic units to (default: kcal/mol).
        time_unit (str): Target unit for time to transform from atomic units to (default: fs).
    """
    # setup paths to the necessary files
    data_prefix = os.path.join(set_data_prefix(), trajectory_dir)
    db_name = "md_trajectory_bohr_hartree_aut.db"
    data_path = os.path.join(data_prefix, db_name)
    split_file = get_split_path(data_prefix, fold=0)

    db = connect(data_path)
    logger.info(f"Connected to database: {data_path}")
    logger.info(f"Database metadata: {db.metadata}")

    data_module = spainn.SPAINN(
        n_states = 2, # singlet states 0, 1
        n_nacs = 1, # couplings: 01
        datapath=data_path, # path to database
        batch_size=100,
        split_file=split_file,
        load_properties=['energy', 'forces', 'smooth_nacs'],
        transforms=[
            # remove mean of energy in every electronic state
            spk.transform.RemoveOffsets("energy", remove_mean=True, remove_atomrefs=False), 
            spk.transform.ASENeighborList(cutoff=5.0),
            spk.transform.CastTo32(),
                ],
    )
    # TODO: this prints one line, like this:
    # {'energy': (tensor([-23.2072, -23.2048], dtype=torch.float64), tensor([0.0004, 0.0005], dtype=torch.float64))}
    # but this energy does not match the energy in the database -> confusing...


    # setup everything
    data_module.prepare_data()
    data_module.setup()
    logger.info(f"Loaded datamodule: {data_module}")
    
    

    properties = data_module.dataset[0]
    print('Loaded properties:\n',
        *[str(i)+'\t'+str(properties[i].shape)+'\n' for i in properties.keys() if not i.startswith('_')])
    
    n_atom_basis = 50
    cutoff = 10.0

    # input module: calculates pairwise distances between atoms
    pairwise_distance = spk.atomistic.PairwiseDistances()

    # radial basis for convolution
    radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)

    painn = spk.representation.PaiNN(
        radial_basis=radial_basis,
        n_atom_basis=n_atom_basis,
        n_interactions=6,
        cutoff_fn=spk.nn.CosineCutoff(cutoff)
    )

    pred_energy = spainn.Atomwise(
        n_in=n_atom_basis,
        n_out=2, # number of electronic states (S0, S1)
        n_layers=3,
    )

    pred_forces = spainn.Forces()

    pred_nacs = spainn.Nacs(
        n_in=n_atom_basis,
        n_out=1, # number of couplings S0 -> S1
        nac_key=spainn.SPAINN.smooth_nacs,
        use_vector_repr=True # False for SchNet
    )

    nnpot = spk.model.NeuralNetworkPotential(
        representation=painn,
        input_modules=[pairwise_distance],
        output_modules=[pred_energy, pred_forces, pred_nacs],
        input_dtype_str='float32',
        do_postprocessing=True,
        postprocessors=[
            spk.transform.CastTo64(),
            spk.transform.AddOffsets(spainn.SPAINN.energy, add_mean=True, add_atomrefs=False),
        ],
    )


    output_energy = spk.task.ModelOutput(
        name=spainn.SPAINN.energy,
        loss_fn=torch.nn.MSELoss(),
        loss_weight=1.0, #0.05,
        metrics={
            "MSE": torchmetrics.MeanSquaredError(),
        },
    )

    output_forces = spk.task.ModelOutput(
        name=spainn.SPAINN.forces,
        loss_fn=torch.nn.MSELoss(),
        loss_weight=1.0, #0.10,
        metrics={
            "MSE": torchmetrics.MeanSquaredError(),
        },
    )

    output_nacs = spk.task.ModelOutput(
        name=spainn.SPAINN.smooth_nacs,
        # MSE loss function for phase properties
        loss_fn=spainn.loss.PhaseLossAtomisticMSE(atoms=48),
        loss_weight=1.0, #0.85,
        metrics={
            "MSE": spainn.metric.PhaseAtomisticMSE(atoms=48),
        },
    )

    import warnings; warnings.simplefilter('ignore')

    task = spk.task.AtomisticTask(
        model=nnpot,
        outputs=[output_energy, output_forces, output_nacs],
        optimizer_cls=torch.optim.AdamW,
        optimizer_args={"lr": 1e-4},
        scheduler_monitor="val_loss",
    )

    modelpath = os.path.join(os.getcwd(), 'train')
    callbacks = [
        spk.train.ModelCheckpoint(
            model_path=os.path.join(modelpath, "best_model_E_F_C"),
            save_top_k=1,
            monitor="val_loss"
        )
    ]

    trainer = pl.Trainer(
        log_every_n_steps=1,
        callbacks=callbacks,
        logger=pl.loggers.TensorBoardLogger(save_dir=modelpath),
        default_root_dir=modelpath,
        max_epochs=1, # for testing, we restrict the number of epochs
    )

    trainer.fit(task, datamodule=data_module)
        
    

if __name__=="__main__":
    args = parse_args()
    import time
    start = time.time()
    main(**args)
    end = time.time()
    logger.debug(f"Total time taken for main(): {end - start:.4f} seconds")

    # remove splitting.lock file if it exists in cwd
    remove_splitting_lock_file()

# using torch.set_float32_matmul_precision(precision) with 5 epochs, 2300 + 300 data points and with batch size 10
# precision='highest': 
# t_total = 43.619317s
# precision='high': 
# t_total = 42.182438s
# precision='medium': 
# t_total = 43.171490s

# using torch.set_float32_matmul_precision(precision) with 5 epochs, 2300 + 300 data points and with batch size 100
# precision='highest': 
# t_total = 13.4122824s
# precision='high': 
# t_total = 13.4929749s
# precision='medium': 
# t_total = 13.2532742s

# => use highest precision for training and batch size 100