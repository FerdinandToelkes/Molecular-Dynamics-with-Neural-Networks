import sys, os
import argparse

from ase.db import connect

import torch
import torchmetrics
import pytorch_lightning as pl
import schnetpack as spk

import spainn

from md_with_schnet.utils import set_data_prefix
from md_with_schnet.setup_logger import setup_logger


logger = setup_logger(logging_level_str="debug")


# Example command to run the script from within code directory:
"""
python -m exited_md.training_and_inference.train_e_f_nac
"""


def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Prepare TDDFT data for usage in SchNetPack")
    parser.add_argument("--target_dir", type=str, default="PREPARE_12", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12)")
    return vars(parser.parse_args())


def main(target_dir: str):
    """
    Main function to prepare XTB data in units of Angstrom and kcal/mol for usage in SchNetPack.
    Args:
        target_dir (str): Directory containing the GEO folders containing exited state trajectories generated by Turbomole.
        computed_cycles (int): Number of cycles for which the gradients were computed.
        num_atoms (int): Number of atoms in the simulation. Can be found in the "control" file under "natoms".
        position_unit (str): Target unit for positions to transform from atomic units to (default: angstrom).
        energy_unit (str): Target unit for energies to transform from atomic units to (default: kcal/mol).
        time_unit (str): Target unit for time to transform from atomic units to (default: fs).
    """
    # setup paths to the necessary files
    data_path = os.path.join(set_data_prefix(), target_dir)
    data_prefix = os.path.join(data_path, "GEO_100000", "test")
    db_name = "md_trajectory_bohr_hartree_aut.db"
    data_path = os.path.join(data_prefix, db_name)

    db = connect(data_path)

    print(db.metadata)

    
    data_module = spainn.SPAINN(
        n_states = 2, # singlet states 0, 1
        n_nacs = 1, # couplings: 01
        datapath=data_path, # path to database
        batch_size=10,
        num_train=0.7, # 70% of databse for training
        num_val=0.1, # 10% of database for validation
        #split_file=os.path.join('train', 'split.npz'), # path to model (training folder)
        splitting=spk.data.splitting.RandomSplit(),
        load_properties=['energy', 'forces', 'nacs'],
        transforms=[
            # remove mean of energy in every electronic state
            spk.transform.RemoveOffsets("energy", remove_mean=True, remove_atomrefs=False), 
            spk.transform.ASENeighborList(cutoff=5.0),
            spk.transform.CastTo32(),
                ],
    )

    # setup everything
    data_module.prepare_data()
    data_module.setup()

    properties = data_module.dataset[0]
    print('Loaded properties:\n',
        *[str(i)+'\t'+str(properties[i].shape)+'\n' for i in properties.keys() if not i.startswith('_')])


    n_atom_basis = 50
    cutoff = 10.0

    # input module: calculates pairwise distances between atoms
    pairwise_distance = spk.atomistic.PairwiseDistances()

    # radial basis for convolution
    radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)

    painn = spk.representation.PaiNN(
        radial_basis=radial_basis,
        n_atom_basis=n_atom_basis,
        n_interactions=6,
        cutoff_fn=spk.nn.CosineCutoff(cutoff)
    )

    pred_energy = spainn.Atomwise(
        n_in=n_atom_basis,
        n_out=2, # number of electronic states (S0, S1)
        n_layers=3,
    )

    pred_forces = spainn.Forces()

    pred_nacs = spainn.Nacs(
        n_in=n_atom_basis,
        n_out=1, # number of couplings S0 -> S1
        nac_key=spainn.SPAINN.nacs,
        use_vector_repr=True # False for SchNet
    )

    nnpot = spk.model.NeuralNetworkPotential(
        representation=painn,
        input_modules=[pairwise_distance],
        output_modules=[pred_energy, pred_forces, pred_nacs],
        input_dtype_str='float32',
        do_postprocessing=True,
        postprocessors=[
            spk.transform.CastTo64(),
            spk.transform.AddOffsets(spainn.SPAINN.energy, add_mean=True, add_atomrefs=False),
        ],
    )


    output_energy = spk.task.ModelOutput(
        name=spainn.SPAINN.energy,
        loss_fn=torch.nn.MSELoss(),
        loss_weight=1.0, #0.05,
        metrics={
            "MSE": torchmetrics.MeanSquaredError(),
        },
    )

    output_forces = spk.task.ModelOutput(
        name=spainn.SPAINN.forces,
        loss_fn=torch.nn.MSELoss(),
        loss_weight=1.0, #0.10,
        metrics={
            "MSE": torchmetrics.MeanSquaredError(),
        },
    )

    output_nacs = spk.task.ModelOutput(
        name=spainn.SPAINN.nacs,
        # MSE loss function for phase properties
        loss_fn=spainn.loss.PhaseLossAtomisticMSE(atoms=48),
        loss_weight=1.0, #0.85,
        metrics={
            "MSE": spainn.metric.PhaseAtomisticMSE(atoms=48),
        },
    )

    import warnings; warnings.simplefilter('ignore')

    task = spk.task.AtomisticTask(
        model=nnpot,
        outputs=[output_energy, output_forces, output_nacs],
        optimizer_cls=torch.optim.AdamW,
        optimizer_args={"lr": 1e-4},
        scheduler_monitor="val_loss",
    )

    modelpath = os.path.join(os.getcwd(), 'train')
    callbacks = [
        spk.train.ModelCheckpoint(
            model_path=os.path.join(modelpath, "best_model_E_F_C"),
            save_top_k=1,
            monitor="val_loss"
        )
    ]

    trainer = pl.Trainer(
        log_every_n_steps=1,
        callbacks=callbacks,
        logger=pl.loggers.TensorBoardLogger(save_dir=modelpath),
        default_root_dir=modelpath,
        max_epochs=1, # for testing, we restrict the number of epochs
    )

    trainer.fit(task, datamodule=data_module)
        
    

if __name__=="__main__":
    args = parse_args()
    main(**args)