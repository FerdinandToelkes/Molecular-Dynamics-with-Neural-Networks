import argparse
import os
import numpy as np

from ground_state_md.setup_logger import setup_logger
from ground_state_md.utils import set_data_prefix
from ground_state_md.preprocessing.create_splits import check_overlap_inner_split, check_overlap_outer_split

logger = setup_logger(logging_level_str="debug")

# Script to generate splits for an inner k-fold cross-validation and save them in a ASE compatible format.
# Example command to run the script from within code directory:
"""
python -m exited_md.preprocessing.create_splits --trajectory_dir PREPARE_12/spainn_datasets --units bohr_hartree_aut
"""

# and for toy data (only first 3 directories):
"""
python -m exited_md.preprocessing.create_splits --trajectory_dir PREPARE_12/spainn_datasets/toy_data --units bohr_hartree_aut --dirs_for_training 1 --dirs_for_validation 1 --dirs_for_testing 1
"""

def parse_args() -> dict:
    """ 
    Parse command-line arguments. 
    Returns:
        dict: Dictionary containing command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Script for generating splits for cross validation.")
    parser.add_argument("--trajectory_dir", type=str, default="PREPARE_12/spainn_datasets", help="Directory containing the trajectory data generated by Turbomole (default: PREPARE_12/spainn_datasets)")
    parser.add_argument("--units", type=str, default="angstrom_kcal_per_mol_fs", choices=["angstrom_kcal_per_mol_fs", "angstrom_ev_fs", "bohr_hartree_aut"], help="Units for the input data (default: angstrom_kcal_per_mol_fs).")
    parser.add_argument("--dirs_for_training", type=int, default=25, help="Number of GEO directories to use for training (default: 25).")
    parser.add_argument("--dirs_for_validation", type=int, default=3, help="Number of GEO directories to use for validation (default: 3).")
    parser.add_argument("--dirs_for_testing", type=int, default=25, help="Number of GEO directories to use for testing (default: 25).")
    parser.add_argument("-k", "--k_fold", type=int, default=5, help="Number of folds for k-fold cross-validation (default: 5).")
    return vars(parser.parse_args())

def get_infos_from_file(info_path: str) -> tuple:
    """ 
    Read the info file to get the number of samples, directories and samples per directory.
    Args:
        info_path (str): Path to the info file.
    Returns:
        tuple: Total number of samples, number of directories, and number of samples per directory.
    Raises:
        FileNotFoundError: If the info file does not exist.
        ValueError: If the info file does not contain the required information.
    """
    if not os.path.exists(info_path):
        raise FileNotFoundError(f"Info file does not exist at {info_path}. Please run the prepare_tddft_data.py script first.")

    with open(info_path, 'r') as f:
        lines = f.readlines()
        for line in lines:
            if "Number of samples in total" in line:
                total_length = int(line.split(":")[1].strip())
            elif "Number of used directories" in line:
                num_dirs = int(line.split(":")[1].strip())
            elif "Number of samples per directory" in line:
                samples_per_geo_dir = int(line.split(":")[1].strip())
        if 'total_length' not in locals() or 'num_dirs' not in locals() or 'samples_per_geo_dir' not in locals():
            raise ValueError("Either 'Number of samples in total', 'Number of used directories' or 'Number of samples per directory' not found in the info file.")

    if total_length != num_dirs * samples_per_geo_dir:
        raise ValueError(f"Total length ({total_length}) does not match number of directories ({num_dirs}) * samples per directory ({samples_per_geo_dir}).")

    logger.info(f"Total length of dataset: {total_length}")
    logger.info(f"Number of directories:   {num_dirs}")
    logger.info(f"Samples per directory:   {samples_per_geo_dir}")

    return total_length, num_dirs, samples_per_geo_dir

    
def main(trajectory_dir: str, units: str, k_fold: int, dirs_for_training: int, dirs_for_validation: int, dirs_for_testing: int):
    # setup the paths to the necessary files
    data_prefix = set_data_prefix()
    splits_dir = os.path.join(data_prefix, trajectory_dir, 'splits')
    os.makedirs(splits_dir, exist_ok=True)
    info_name = f"md_trajectory_{units}_info.txt"
    info_path = os.path.join(data_prefix, trajectory_dir, info_name)
    
    # read the info file to get the number of samples, directories and samples per directory
    total_length, num_dirs, samples_per_geo_dir = get_infos_from_file(info_path)


    if dirs_for_training + dirs_for_validation + dirs_for_testing != num_dirs:
        raise ValueError(f"Sum of directories for training ({dirs_for_training}), validation ({dirs_for_validation}), and testing ({dirs_for_testing}) does not equal number of directories ({num_dirs}).")
    
    # create outer split - train data = dirs_for_training + dirs_for_validation, test data = dirs_for_testing
    outer_train_split = (dirs_for_training + dirs_for_validation) * samples_per_geo_dir
    outer_train_indices = list(range(outer_train_split))
    outer_test_indices = list(range(outer_train_split, total_length))
    logger.debug(f"Outer train indices[:10]: {outer_train_indices[:10]}")
    logger.debug(f"Outer test indices[:10]: {outer_test_indices[:10]}")
    
    # save outer splits as npz file
    outer_splits = {
        "train": outer_train_indices,
        "test": outer_test_indices
    }
    outer_splits_path = os.path.join(splits_dir, "outer_splits.npz")
    check_overlap_outer_split(outer_train_indices, outer_test_indices)

    if os.path.exists(outer_splits_path):
        raise FileExistsError(f"Outer splits already exist at {outer_splits_path}. Please remove it to create new splits.")
    np.savez(outer_splits_path, **outer_splits)
    logger.info(f"Saved outer splits to: {outer_splits_path}")

    # create inner splits - k folds each dirs_for_training for training, dirs_for_validation for validation
    inner_val_split = dirs_for_validation * samples_per_geo_dir
    for k in range(k_fold):  
        inner_val_indices = outer_train_indices[k*inner_val_split : (k+1)*inner_val_split]
        inner_train_indices = [i for i in outer_train_indices if i not in inner_val_indices]
        # save train, val and test indices as npz file
        inner_splits = {
            "train_idx": inner_train_indices, # naming important for ASE and SPaiNN
            "val_idx": inner_val_indices,
            "test_idx": outer_test_indices
        }

        # check if they overlap
        check_overlap_inner_split(inner_train_indices, inner_val_indices, outer_test_indices)

        inner_splits_path = os.path.join(splits_dir, f"inner_splits_{k}.npz")
        np.savez(inner_splits_path, **inner_splits)
        logger.info(f"Saved inner splits from fold {k} to: {inner_splits_path}")
    

        

if __name__=="__main__":
    args = parse_args()
    main(**args)